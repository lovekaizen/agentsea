1:"$Sreact.fragment"
2:I[1275,["619","static/chunks/619-ba102abea3e3d0e4.js","543","static/chunks/543-a0960cd7f2b1c22c.js","177","static/chunks/app/layout-b0f860a3d9150669.js"],"default"]
3:I[9766,[],""]
4:I[8924,[],""]
32:I[7150,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/7e7d96b1e6991756.css","style"]
:HL["/_next/static/css/1145ad3a91ffb5c9.css","style"]
:HL["/_next/static/css/7a622664f4c377b7.css","style"]
0:{"P":null,"b":"BR2GXXFimfBinRJoMr3YY","p":"","c":["","docs","local-models",""],"i":false,"f":[[["",{"children":["docs",{"children":["local-models",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/7e7d96b1e6991756.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/1145ad3a91ffb5c9.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","2",{"rel":"stylesheet","href":"/_next/static/css/7a622664f4c377b7.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","data-theme":"cmyk","children":["$","body",null,{"className":"__className_f367f3","children":["$","$L2",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]}]]}],{"children":["docs",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["local-models",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"min-h-screen bg-base-100","children":["$","div",null,{"className":"container mx-auto px-6 py-12 max-w-5xl ","children":[["$","div",null,{"className":"mb-12","children":[["$","h1",null,{"className":"text-5xl font-bold mb-4","children":"Local Models & Open Source Providers"}],["$","p",null,{"className":"text-xl text-base-content/70","children":"Run AI agents completely locally with open source models - perfect for privacy, cost savings, and offline development."}]]}],["$","div",null,{"className":"prose max-w-none","children":[["$","div",null,{"className":"bg-green-50 border border-green-200 rounded-lg p-6 my-8","children":[["$","h3",null,{"className":"text-lg font-semibold text-green-900 mb-2","children":"üîí Complete Privacy & Zero API Costs"}],["$","div",null,{"className":"text-green-800 space-y-2","children":[["$","p",null,{"className":"m-0","children":"‚úÖ Your data never leaves your machine"}],["$","p",null,{"className":"m-0","children":"‚úÖ No API keys required"}],["$","p",null,{"className":"m-0","children":"‚úÖ Works offline"}],["$","p",null,{"className":"m-0","children":"‚úÖ Unlimited usage - no per-token costs"}],["$","p",null,{"className":"m-0","children":"‚úÖ 6 local providers supported: Ollama, LM Studio, LocalAI, Text Generation WebUI, vLLM, Jan"}]]}]]}],["$","h2",null,{"id":"overview","children":"Why Local Models?"}],["$","div",null,{"className":"grid md:grid-cols-3 gap-4 my-6","children":[["$","div",null,{"className":"border border-gray-200 rounded-lg p-4","children":[["$","div",null,{"className":"text-2xl mb-2","children":"üîí"}],["$","h4",null,{"className":"font-semibold mb-2","children":"Privacy"}],["$","p",null,{"className":"text-sm m-0","children":"Sensitive data stays on your infrastructure. Perfect for healthcare, finance, legal."}]]}],["$","div",null,{"className":"border border-gray-200 rounded-lg p-4","children":["$L5","$L6","$L7"]}],"$L8"]}],"$L9","$La","$Lb","$Lc","$Ld","$Le","$Lf","$L10","$L11","$L12","$L13","$L14","$L15","$L16","$L17","$L18","$L19","$L1a","$L1b","$L1c","$L1d","$L1e","$L1f","$L20","$L21","$L22","$L23","$L24","$L25","$L26","$L27","$L28","$L29","$L2a","$L2b","$L2c","$L2d","$L2e","$L2f"]}]]}]}],null,"$L30"]}],{},null,false]},null,false]},null,false]},null,false],"$L31",false]],"m":"$undefined","G":["$32",[]],"s":false,"S":true}
33:I[8467,["619","static/chunks/619-ba102abea3e3d0e4.js","501","static/chunks/501-26489c9c153fdafc.js","50","static/chunks/app/docs/local-models/page-1661e6a98b49e8b6.js"],"CodeBlock"]
35:I[2619,["619","static/chunks/619-ba102abea3e3d0e4.js","501","static/chunks/501-26489c9c153fdafc.js","50","static/chunks/app/docs/local-models/page-1661e6a98b49e8b6.js"],""]
36:I[4431,[],"OutletBoundary"]
38:I[5278,[],"AsyncMetadataOutlet"]
3a:I[4431,[],"ViewportBoundary"]
3c:I[4431,[],"MetadataBoundary"]
3d:"$Sreact.suspense"
5:["$","div",null,{"className":"text-2xl mb-2","children":"üí∞"}]
6:["$","h4",null,{"className":"font-semibold mb-2","children":"Cost Savings"}]
7:["$","p",null,{"className":"text-sm m-0","children":"No per-token charges. Save $75K+ annually on API costs for production apps."}]
8:["$","div",null,{"className":"border border-gray-200 rounded-lg p-4","children":[["$","div",null,{"className":"text-2xl mb-2","children":"‚ö°"}],["$","h4",null,{"className":"font-semibold mb-2","children":"Control"}],["$","p",null,{"className":"text-sm m-0","children":"Full control over models, versions, and infrastructure. No rate limits."}]]}]
9:["$","h2",null,{"id":"ollama","children":"Ollama (Recommended)"}]
a:["$","p",null,{"children":["The easiest way to run local models. Ollama makes running LLMs as simple as ",["$","code",null,{"children":"ollama pull llama3.2"}],"."]}]
b:["$","h3",null,{"children":"Quick Start"}]
c:["$","$L33",null,{"language":"bash","children":"# Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Pull a model\nollama pull llama3.2\n\n# Run a model\nollama run llama3.2"}]
d:["$","h3",null,{"children":"Using with AgentSea ADK"}]
e:["$","$L33",null,{"language":"typescript","children":"import {\n  Agent,\n  OllamaProvider,\n  ToolRegistry,\n  BufferMemory,\n  calculatorTool,\n} from '@lov3kaizen/agentsea-core';\n\n// Create Ollama provider - no API key needed!\nconst provider = new OllamaProvider({\n  baseUrl: 'http://localhost:11434',\n});\n\n// Check available models\nconst models = await provider.listModels();\nconsole.log('Available models:', models);\n\n// Pull a new model if needed\nif (!models.includes('llama3.2')) {\n  console.log('Pulling llama3.2...');\n  await provider.pullModel('llama3.2');\n}\n\n// Create agent\nconst agent = new Agent(\n  {\n    name: 'local-assistant',\n    model: 'llama3.2',\n    provider: 'ollama',\n    systemPrompt: 'You are a helpful assistant running locally.',\n    tools: [calculatorTool],\n    temperature: 0.7,\n    maxTokens: 2048,\n  },\n  provider,\n  new ToolRegistry(),\n  new BufferMemory(50),\n);\n\n// Use it - everything runs locally!\nconst response = await agent.execute('What is 42 * 58?', {\n  conversationId: 'local-user',\n  sessionData: {},\n  history: [],\n});\n\nconsole.log(response.content);"}]
f:["$","h3",null,{"children":"Popular Models for Ollama"}]
10:["$","div",null,{"className":"overflow-x-auto my-6","children":["$","table",null,{"className":"min-w-full border border-gray-200 text-sm","children":[["$","thead",null,{"className":"bg-gray-100","children":["$","tr",null,{"children":[["$","th",null,{"className":"px-4 py-2 text-left","children":"Model"}],["$","th",null,{"className":"px-4 py-2 text-left","children":"Size"}],["$","th",null,{"className":"px-4 py-2 text-left","children":"RAM"}],["$","th",null,{"className":"px-4 py-2 text-left","children":"Best For"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"className":"border-t border-gray-200","children":[["$","td",null,{"className":"px-4 py-2","children":["$","strong",null,{"children":"llama3.2:3b"}]}],["$","td",null,{"className":"px-4 py-2","children":"2GB"}],["$","td",null,{"className":"px-4 py-2","children":"8GB"}],["$","td",null,{"className":"px-4 py-2","children":"Fast, lightweight, good quality"}]]}],["$","tr",null,{"className":"border-t border-gray-200","children":[["$","td",null,{"className":"px-4 py-2","children":["$","strong",null,{"children":"llama3.2:latest"}]}],["$","td",null,{"className":"px-4 py-2","children":"4.7GB"}],["$","td",null,{"className":"px-4 py-2","children":"16GB"}],["$","td",null,{"className":"px-4 py-2","children":"Best balance of quality & speed"}]]}],["$","tr",null,{"className":"border-t border-gray-200","children":[["$","td",null,{"className":"px-4 py-2","children":["$","strong",null,{"children":"mistral"}]}],["$","td",null,{"className":"px-4 py-2","children":"4.1GB"}],["$","td",null,{"className":"px-4 py-2","children":"16GB"}],["$","td",null,{"className":"px-4 py-2","children":"Excellent instruction following"}]]}],["$","tr",null,{"className":"border-t border-gray-200","children":[["$","td",null,{"className":"px-4 py-2","children":["$","strong",null,{"children":"qwen2.5"}]}],["$","td",null,{"className":"px-4 py-2","children":"4.7GB"}],["$","td",null,{"className":"px-4 py-2","children":"16GB"}],["$","td",null,{"className":"px-4 py-2","children":"Strong coding & reasoning"}]]}],["$","tr",null,{"className":"border-t border-gray-200","children":[["$","td",null,{"className":"px-4 py-2","children":["$","strong",null,{"children":"gemma2"}]}],["$","td",null,{"className":"px-4 py-2","children":"5.4GB"}],["$","td",null,{"className":"px-4 py-2","children":"16GB"}],["$","td",null,{"className":"px-4 py-2","children":"Google's open model"}]]}],["$","tr",null,{"className":"border-t border-gray-200","children":[["$","td",null,{"className":"px-4 py-2","children":["$","strong",null,{"children":"codellama"}]}],["$","td",null,{"className":"px-4 py-2","children":"3.8GB"}],["$","td",null,{"className":"px-4 py-2","children":"16GB"}],["$","td",null,{"className":"px-4 py-2","children":"Code generation"}]]}]]}]]}]}]
11:["$","h2",null,{"id":"other-providers","children":"Other Local Providers"}]
12:["$","h3",null,{"children":"LM Studio"}]
13:["$","p",null,{"children":"Desktop app with beautiful UI for running local models. OpenAI-compatible API server included."}]
14:["$","$L33",null,{"language":"typescript","children":"import { LMStudioProvider } from '@lov3kaizen/agentsea-core';\n\nconst provider = new LMStudioProvider({\n  baseUrl: 'http://localhost:1234',\n});\n\n// Use like any other provider\nconst agent = new Agent(\n  {\n    name: 'lm-studio-assistant',\n    model: 'local-model', // Model loaded in LM Studio\n    provider: 'lm-studio',\n  },\n  provider,\n  toolRegistry,\n);"}]
15:["$","h3",null,{"children":"LocalAI"}]
16:["$","p",null,{"children":"Self-hosted OpenAI alternative supporting LLMs, Stable Diffusion, voice, embeddings."}]
17:["$","$L33",null,{"language":"typescript","children":"import { LocalAIProvider } from '@lov3kaizen/agentsea-core';\n\nconst provider = new LocalAIProvider({\n  baseUrl: 'http://localhost:8080',\n});\n\nconst agent = new Agent(\n  {\n    name: 'localai-assistant',\n    model: 'llama-3.2-3b',\n    provider: 'localai',\n  },\n  provider,\n  toolRegistry,\n);"}]
18:["$","h3",null,{"children":"Text Generation WebUI"}]
19:["$","p",null,{"children":"Feature-rich web UI for running models with extensions ecosystem."}]
1a:["$","$L33",null,{"language":"typescript","children":"import { TextGenerationWebUIProvider } from '@lov3kaizen/agentsea-core';\n\nconst provider = new TextGenerationWebUIProvider({\n  baseUrl: 'http://localhost:5000',\n});"}]
1b:["$","h3",null,{"children":"vLLM"}]
1c:["$","p",null,{"children":"High-throughput inference server for production deployments. Uses PagedAttention for efficiency."}]
1d:["$","$L33",null,{"language":"typescript","children":"import { VLLMProvider } from '@lov3kaizen/agentsea-core';\n\nconst provider = new VLLMProvider({\n  baseUrl: 'http://localhost:8000',\n});"}]
1e:["$","h2",null,{"id":"model-management","children":"Model Management"}]
1f:["$","p",null,{"children":"Ollama provider includes built-in model management:"}]
20:["$","$L33",null,{"language":"typescript","children":"const provider = new OllamaProvider();\n\n// List available models\nconst models = await provider.listModels();\nconsole.log(models); // ['llama3.2', 'mistral', ...]\n\n// Pull a new model\nawait provider.pullModel('codellama');\n\n// Use the model\nconst agent = new Agent(\n  {\n    model: 'codellama',\n    provider: 'ollama',\n  },\n  provider,\n  toolRegistry,\n);\n\n// Model info\nconst info = await provider.getModelInfo('codellama');\nconsole.log('Model size:', info.size);\nconsole.log('Parameters:', info.parameters);"}]
21:["$","h2",null,{"id":"streaming","children":"Streaming Support"}]
22:["$","p",null,{"children":"All local providers support streaming for real-time responses:"}]
23:["$","$L33",null,{"language":"typescript","children":"import { Agent, OllamaProvider } from '@lov3kaizen/agentsea-core';\n\nconst provider = new OllamaProvider();\nconst agent = new Agent(\n  {\n    model: 'llama3.2',\n    provider: 'ollama',\n    stream: true, // Enable streaming\n  },\n  provider,\n  toolRegistry,\n);\n\n// Stream response chunks\nfor await (const chunk of agent.stream('Write a story', context)) {\n  process.stdout.write(chunk.content);\n}"}]
24:["$","h2",null,{"id":"complete-privacy","children":"Complete Privacy Example"}]
25:["$","p",null,{"children":"Build a fully private AI system - LLM, voice, and tools all running locally:"}]
34:T439,import {
  Agent,
  OllamaProvider,
  VoiceAgent,
  LocalWhisperProvider,
  PiperTTSProvider,
  ToolRegistry,
  BufferMemory,
} from '@lov3kaizen/agentsea-core';

// Local LLM
const ollamaProvider = new OllamaProvider();

// Local voice
const sttProvider = new LocalWhisperProvider({
  whisperPath: '/usr/local/bin/whisper',
  modelPath: '/path/to/ggml-base.bin',
});

const ttsProvider = new PiperTTSProvider({
  piperPath: '/usr/local/bin/piper',
  modelPath: '/path/to/en_US-lessac-medium.onnx',
});

// Create agent
const agent = new Agent(
  {
    name: 'private-assistant',
    model: 'llama3.2',
    provider: 'ollama',
    systemPrompt: 'You are a completely private AI assistant.',
  },
  ollamaProvider,
  new ToolRegistry(),
  new BufferMemory(100),
);

// Wrap with voice
const voiceAgent = new VoiceAgent(agent, {
  sttProvider,
  ttsProvider,
  autoSpeak: true,
});

// Everything runs locally - complete privacy!
const result = await voiceAgent.processVoice(audioInput, context);

// ‚úÖ No data sent to cloud
// ‚úÖ No API keys needed
// ‚úÖ Works offline
// ‚úÖ Zero API costs26:["$","$L33",null,{"language":"typescript","children":"$34"}]
27:["$","h2",null,{"id":"performance","children":"Performance Tips"}]
28:["$","div",null,{"className":"space-y-4 my-6","children":[["$","div",null,{"className":"bg-blue-50 border border-blue-200 rounded-lg p-4","children":[["$","h4",null,{"className":"text-blue-900 font-semibold mb-2","children":"üöÄ GPU Acceleration"}],["$","p",null,{"className":"text-blue-800 text-sm m-0","children":"Ollama automatically uses GPU if available. Expect 10-50x faster inference with NVIDIA GPU."}]]}],["$","div",null,{"className":"bg-purple-50 border border-purple-200 rounded-lg p-4","children":[["$","h4",null,{"className":"text-purple-900 font-semibold mb-2","children":"üíæ Model Size vs Quality"}],["$","p",null,{"className":"text-purple-800 text-sm m-0","children":"Start with 3B models (8GB RAM) for testing. Use 7B models (16GB RAM) for production quality."}]]}],["$","div",null,{"className":"bg-green-50 border border-green-200 rounded-lg p-4","children":[["$","h4",null,{"className":"text-green-900 font-semibold mb-2","children":"‚ö° Context Length"}],["$","p",null,{"className":"text-green-800 text-sm m-0","children":"Reduce max_tokens for faster responses. Most conversations work well with 1024-2048 tokens."}]]}],["$","div",null,{"className":"bg-orange-50 border border-orange-200 rounded-lg p-4","children":[["$","h4",null,{"className":"text-orange-900 font-semibold mb-2","children":"üîÑ Keep Models Loaded"}],["$","p",null,{"className":"text-orange-800 text-sm m-0","children":"Ollama keeps models in memory for 5 minutes after use. First request loads model (slow), subsequent requests are fast."}]]}]]}]
29:["$","h2",null,{"id":"provider-comparison","children":"Provider Comparison"}]
2a:["$","div",null,{"className":"overflow-x-auto my-6","children":["$","table",null,{"className":"min-w-full border border-gray-200 text-sm","children":[["$","thead",null,{"className":"bg-gray-100","children":["$","tr",null,{"children":[["$","th",null,{"className":"px-4 py-2 text-left","children":"Provider"}],["$","th",null,{"className":"px-4 py-2 text-left","children":"Ease of Use"}],["$","th",null,{"className":"px-4 py-2 text-left","children":"Performance"}],["$","th",null,{"className":"px-4 py-2 text-left","children":"Features"}],["$","th",null,{"className":"px-4 py-2 text-left","children":"Best For"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"className":"border-t border-gray-200","children":[["$","td",null,{"className":"px-4 py-2","children":["$","strong",null,{"children":"Ollama"}]}],["$","td",null,{"className":"px-4 py-2","children":"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-4 py-2","children":"‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-4 py-2","children":"Model mgmt, CLI"}],["$","td",null,{"className":"px-4 py-2","children":"Getting started, development"}]]}],["$","tr",null,{"className":"border-t border-gray-200","children":[["$","td",null,{"className":"px-4 py-2","children":["$","strong",null,{"children":"LM Studio"}]}],["$","td",null,{"className":"px-4 py-2","children":"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-4 py-2","children":"‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-4 py-2","children":"GUI, easy setup"}],["$","td",null,{"className":"px-4 py-2","children":"Non-technical users"}]]}],["$","tr",null,{"className":"border-t border-gray-200","children":[["$","td",null,{"className":"px-4 py-2","children":["$","strong",null,{"children":"LocalAI"}]}],["$","td",null,{"className":"px-4 py-2","children":"‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-4 py-2","children":"‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-4 py-2","children":"Multi-modal, Docker"}],["$","td",null,{"className":"px-4 py-2","children":"Self-hosted services"}]]}],["$","tr",null,{"className":"border-t border-gray-200","children":[["$","td",null,{"className":"px-4 py-2","children":["$","strong",null,{"children":"vLLM"}]}],["$","td",null,{"className":"px-4 py-2","children":"‚≠ê‚≠ê"}],["$","td",null,{"className":"px-4 py-2","children":"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-4 py-2","children":"PagedAttention"}],["$","td",null,{"className":"px-4 py-2","children":"Production, high throughput"}]]}],["$","tr",null,{"className":"border-t border-gray-200","children":[["$","td",null,{"className":"px-4 py-2","children":["$","strong",null,{"children":"Text Gen WebUI"}]}],["$","td",null,{"className":"px-4 py-2","children":"‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-4 py-2","children":"‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-4 py-2","children":"Web UI, extensions"}],["$","td",null,{"className":"px-4 py-2","children":"Experimentation"}]]}]]}]]}]}]
2b:["$","h2",null,{"id":"use-cases","children":"Use Cases"}]
2c:["$","div",null,{"className":"grid md:grid-cols-2 gap-4 my-6","children":[["$","div",null,{"className":"border border-gray-200 rounded-lg p-4","children":[["$","h4",null,{"className":"font-semibold mb-2","children":"üè• Healthcare"}],["$","p",null,{"className":"text-sm m-0","children":"Process patient data locally, maintain HIPAA compliance without cloud dependencies."}]]}],["$","div",null,{"className":"border border-gray-200 rounded-lg p-4","children":[["$","h4",null,{"className":"font-semibold mb-2","children":"üí∞ Finance"}],["$","p",null,{"className":"text-sm m-0","children":"Analyze financial data on-premise, meet regulatory requirements for data sovereignty."}]]}],["$","div",null,{"className":"border border-gray-200 rounded-lg p-4","children":[["$","h4",null,{"className":"font-semibold mb-2","children":"‚öñÔ∏è Legal"}],["$","p",null,{"className":"text-sm m-0","children":"Review confidential documents locally, maintain attorney-client privilege."}]]}],["$","div",null,{"className":"border border-gray-200 rounded-lg p-4","children":[["$","h4",null,{"className":"font-semibold mb-2","children":"üöÄ Startups"}],["$","p",null,{"className":"text-sm m-0","children":"Build MVP without API costs, scale without per-token charges eating profits."}]]}]]}]
2d:["$","h2",null,{"id":"next-steps","children":"Next Steps"}]
2e:["$","ul",null,{"children":[["$","li",null,{"children":[["$","$L35",null,{"href":"/docs/voice","children":"Add Voice Features"}]," - Local voice with Whisper & Piper"]}],["$","li",null,{"children":[["$","$L35",null,{"href":"/docs/cli","children":"Use the CLI Tool"}]," - Interactive model management"]}],["$","li",null,{"children":[["$","$L35",null,{"href":"/docs/providers","children":"Explore All Providers"}]," - 12+ providers total"]}],["$","li",null,{"children":[["$","$L35",null,{"href":"/examples","children":"View Examples"}]," - Complete local examples"]}]]}]
2f:["$","div",null,{"className":"bg-gradient-to-r from-green-50 to-blue-50 border border-green-200 rounded-lg p-6 mt-8","children":[["$","h3",null,{"className":"text-lg font-semibold text-green-900 mb-2","children":"üí° Recommended Setup"}],["$","p",null,{"className":"text-green-800 m-0","children":[["$","strong",null,{"children":"Development:"}]," Start with Ollama + llama3.2:3b (fast, good quality)",["$","br",null,{}],["$","strong",null,{"children":"Production:"}]," Use vLLM + mistral-7b (best throughput)",["$","br",null,{}],["$","strong",null,{"children":"Privacy:"}]," Ollama + Local Whisper + Piper TTS (100% local)"]}]]}]
30:["$","$L36",null,{"children":["$L37",["$","$L38",null,{"promise":"$@39"}]]}]
31:["$","$1","h",{"children":[null,[["$","$L3a",null,{"children":"$L3b"}],["$","meta",null,{"name":"next-size-adjust","content":""}]],["$","$L3c",null,{"children":["$","div",null,{"hidden":true,"children":["$","$3d",null,{"fallback":null,"children":"$L3e"}]}]}]]}]
3b:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
37:null
39:{"metadata":[["$","title","0",{"children":"AgentSea - Unite and Orchestrate AI Agents"}],["$","meta","1",{"name":"description","content":"Build powerful agentic AI applications with AgentSea. Unite AI agents and services with multi-provider support, workflow orchestration, MCP protocol, and enterprise-grade observability."}]],"error":null,"digest":"$undefined"}
3e:"$39:metadata"
