<!doctype html>
<!--BR2GXXFimfBinRJoMr3YY-->
<html lang="en" data-theme="cmyk">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="preload"
      href="/_next/static/media/e4af272ccee01ff0-s.p.woff2"
      as="font"
      crossorigin=""
      type="font/woff2"
    />
    <link
      rel="stylesheet"
      href="/_next/static/css/7e7d96b1e6991756.css"
      data-precedence="next"
    />
    <link
      rel="stylesheet"
      href="/_next/static/css/1145ad3a91ffb5c9.css"
      data-precedence="next"
    />
    <link
      rel="stylesheet"
      href="/_next/static/css/7a622664f4c377b7.css"
      data-precedence="next"
    />
    <link
      rel="preload"
      as="script"
      fetchpriority="low"
      href="/_next/static/chunks/webpack-1b7e2f9af6e0232d.js"
    />
    <script
      src="/_next/static/chunks/4bd1b696-c023c6e3521b1417.js"
      async=""
    ></script>
    <script
      src="/_next/static/chunks/255-0fb808ac7cfa018d.js"
      async=""
    ></script>
    <script
      src="/_next/static/chunks/main-app-55a8534c92ac85d5.js"
      async=""
    ></script>
    <script
      src="/_next/static/chunks/619-ba102abea3e3d0e4.js"
      async=""
    ></script>
    <script
      src="/_next/static/chunks/543-a0960cd7f2b1c22c.js"
      async=""
    ></script>
    <script
      src="/_next/static/chunks/app/layout-b0f860a3d9150669.js"
      async=""
    ></script>
    <meta name="next-size-adjust" content="" />
    <title>AgentSea - Unite and Orchestrate AI Agents</title>
    <meta
      name="description"
      content="Build powerful agentic AI applications with AgentSea. Unite AI agents and services with multi-provider support, workflow orchestration, MCP protocol, and enterprise-grade observability."
    />
    <script
      src="/_next/static/chunks/polyfills-42372ed130431b0a.js"
      nomodule=""
    ></script>
  </head>
  <body class="__className_f367f3">
    <div hidden=""><!--$--><!--/$--></div>
    <div class="drawer lg:drawer-open">
      <input id="main-drawer" type="checkbox" class="drawer-toggle" />
      <div class="drawer-content flex flex-col">
        <div class="navbar">
          <div class="flex-none lg:hidden">
            <label for="main-drawer" class="btn btn-square btn-ghost"
              ><svg
                xmlns="http://www.w3.org/2000/svg"
                fill="none"
                viewBox="0 0 24 24"
                class="inline-block w-6 h-6 stroke-current"
              >
                <path
                  stroke-linecap="round"
                  stroke-linejoin="round"
                  stroke-width="2"
                  d="M4 6h16M4 12h16M4 18h16"
                ></path></svg
            ></label>
          </div>
          <div class="flex-1">
            <a class="btn btn-ghost px-0 py-6" href="/"
              ><img
                alt="AgentSea Logo"
                loading="lazy"
                width="217"
                height="60"
                decoding="async"
                data-nimg="1"
                style="color: transparent"
                src="/agentsea-sdk-logo.svg"
            /></a>
          </div>
          <div class="flex-none">
            <a
              href="https://github.com/lovekaizen/agentsea"
              target="_blank"
              rel="noopener noreferrer"
              class="btn btn-ghost btn-circle"
              title="AgentSea on Github"
              ><svg
                xmlns="http://www.w3.org/2000/svg"
                width="24"
                height="24"
                viewBox="0 0 24 24"
                fill="currentColor"
              >
                <path
                  d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"
                ></path></svg
            ></a>
          </div>
        </div>
        <main class="flex-1">
          <div class="min-h-screen bg-base-100">
            <div class="container mx-auto px-6 py-12 max-w-5xl">
              <div class="mb-12">
                <h1 class="text-5xl font-bold mb-4">
                  Local &amp; Open Source Providers
                </h1>
                <p class="text-xl text-base-content/70">
                  Run AI agents completely locally or with open source models.
                  Perfect for privacy-sensitive applications, offline
                  deployments, cost optimization, and development.
                </p>
              </div>
              <div class="prose prose-lg max-w-none">
                <div class="bg-slate-50 border-l-4 border-blue-500 p-6 mb-8">
                  <h3 class="text-lg font-semibold mb-2">
                    Why Use Local Providers?
                  </h3>
                  <ul class="space-y-2">
                    <li>
                      üîí <strong>Privacy &amp; Security</strong> - Data never
                      leaves your infrastructure
                    </li>
                    <li>
                      üí∞ <strong>Cost Savings</strong> - No per-token API costs
                    </li>
                    <li>
                      ‚ö° <strong>Low Latency</strong> - No network round trips
                      for inference
                    </li>
                    <li>
                      üîå <strong>Offline Capable</strong> - Works without
                      internet connection
                    </li>
                    <li>
                      üéõÔ∏è <strong>Full Control</strong> - Customize models,
                      parameters, and deployment
                    </li>
                  </ul>
                </div>
                <h2 class="text-3xl font-bold mt-12 mb-4">
                  Supported Local Providers
                </h2>
                <div class="border rounded-lg p-6 mb-6 bg-slate-50 shadow-sm">
                  <h3 class="text-2xl font-bold mb-3">ü¶ô Ollama</h3>
                  <p class="mb-4">
                    The easiest way to run local LLMs. Supports Llama 3,
                    Mistral, Gemma, and 50+ other models with automatic model
                    management and GPU acceleration.
                  </p>
                  <h4 class="text-lg font-semibold mb-2">Installation</h4>
                  <pre
                    class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4"
                  ><code># macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# Download from https://ollama.com/download

# Pull a model
ollama pull llama3.2
ollama pull mistral
ollama pull gemma2</code></pre>
                  <h4 class="text-lg font-semibold mb-2">Basic Usage</h4>
                  <pre
                    class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4"
                  ><code>import { Agent, OllamaProvider, ToolRegistry } from &#x27;@lov3kaizen/agentsea-core&#x27;;

// Initialize Ollama provider
const provider = new OllamaProvider({
  baseUrl: &#x27;http://localhost:11434&#x27;,
  model: &#x27;llama3.2&#x27; // or &#x27;mistral&#x27;, &#x27;gemma2&#x27;, etc.
});

// Create agent with local model
const agent = new Agent(
  {
    name: &#x27;local-assistant&#x27;,
    model: &#x27;llama3.2&#x27;,
    provider: &#x27;ollama&#x27;,
    systemPrompt: &#x27;You are a helpful AI assistant running locally.&#x27;,
    tools: [],
    temperature: 0.7
  },
  provider,
  new ToolRegistry()
);

// Execute locally
const response = await agent.execute(
  &#x27;What are the benefits of running AI models locally?&#x27;
);

console.log(response.content);</code></pre>
                  <h4 class="text-lg font-semibold mb-2">Recommended Models</h4>
                  <ul class="list-disc pl-6 mb-4 space-y-2">
                    <li>
                      <strong>llama3.2:3b</strong> - Fast, efficient, great for
                      chat (3GB RAM)
                    </li>
                    <li>
                      <strong>llama3.1:8b</strong> - Balanced performance and
                      quality (8GB RAM)
                    </li>
                    <li>
                      <strong>mistral:7b</strong> - Excellent instruction
                      following (7GB RAM)
                    </li>
                    <li>
                      <strong>gemma2:9b</strong> - Google&#x27;s efficient model
                      (9GB RAM)
                    </li>
                    <li>
                      <strong>qwen2.5:7b</strong> - Strong multilingual support
                      (7GB RAM)
                    </li>
                  </ul>
                </div>
                <div class="border rounded-lg p-6 mb-6 bg-slate-50 shadow-sm">
                  <h3 class="text-2xl font-bold mb-3">üöÄ llama.cpp</h3>
                  <p class="mb-4">
                    High-performance C++ inference engine with Metal (Mac), CUDA
                    (NVIDIA), and CPU support. Provides the fastest local
                    inference with quantized models.
                  </p>
                  <h4 class="text-lg font-semibold mb-2">Installation</h4>
                  <pre
                    class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4"
                  ><code># Build from source
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# macOS (with Metal acceleration)
make LLAMA_METAL=1

# Linux with CUDA
make LLAMA_CUDA=1

# Start server
./llama-server \
  -m models/llama-3.2-3b-q4_k_m.gguf \
  --port 8080 \
  --n-gpu-layers 35</code></pre>
                  <h4 class="text-lg font-semibold mb-2">Basic Usage</h4>
                  <pre
                    class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4"
                  ><code>import { Agent, LlamaCppProvider, ToolRegistry } from &#x27;@lov3kaizen/agentsea-core&#x27;;

// Initialize llama.cpp provider
const provider = new LlamaCppProvider({
  baseUrl: &#x27;http://localhost:8080&#x27;,
  model: &#x27;llama-3.2-3b-q4_k_m&#x27;
});

const agent = new Agent(
  {
    name: &#x27;fast-agent&#x27;,
    model: &#x27;llama-3.2-3b-q4_k_m&#x27;,
    provider: &#x27;llama-cpp&#x27;,
    systemPrompt: &#x27;You are a fast, efficient AI assistant.&#x27;,
    tools: [],
    temperature: 0.7,
    maxTokens: 2048
  },
  provider,
  new ToolRegistry()
);

// Stream responses for real-time output
const stream = await agent.stream(&#x27;Explain quantum computing&#x27;);

for await (const chunk of stream) {
  if (chunk.type === &#x27;content&#x27;) {
    process.stdout.write(chunk.content);
  }
}</code></pre>
                  <h4 class="text-lg font-semibold mb-2">
                    Download Quantized Models
                  </h4>
                  <p class="mb-2">
                    Get pre-quantized GGUF models from HuggingFace:
                  </p>
                  <ul class="list-disc pl-6 mb-4 space-y-1">
                    <li>
                      <a
                        href="https://huggingface.co/bartowski"
                        class="text-blue-600 hover:underline"
                        >bartowski&#x27;s collection</a
                      >
                      - Wide variety of quantized models
                    </li>
                    <li>
                      <a
                        href="https://huggingface.co/TheBloke"
                        class="text-blue-600 hover:underline"
                        >TheBloke&#x27;s collection</a
                      >
                      - Popular GGUF conversions
                    </li>
                    <li>
                      <a
                        href="https://huggingface.co/QuantFactory"
                        class="text-blue-600 hover:underline"
                        >QuantFactory</a
                      >
                      - High-quality quantizations
                    </li>
                  </ul>
                </div>
                <div class="border rounded-lg p-6 mb-6 bg-slate-50 shadow-sm">
                  <h3 class="text-2xl font-bold mb-3">üåê GPT4All</h3>
                  <p class="mb-4">
                    Privacy-focused local LLM platform with easy-to-use desktop
                    app and Python/TypeScript bindings. Great for beginners and
                    non-technical users.
                  </p>
                  <h4 class="text-lg font-semibold mb-2">Installation</h4>
                  <pre
                    class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4"
                  ><code># Install GPT4All package
npm install gpt4all

# Or download desktop app
# https://gpt4all.io/</code></pre>
                  <h4 class="text-lg font-semibold mb-2">Basic Usage</h4>
                  <pre
                    class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4"
                  ><code>import { Agent, GPT4AllProvider, ToolRegistry } from &#x27;@lov3kaizen/agentsea-core&#x27;;

// Initialize GPT4All provider
const provider = new GPT4AllProvider({
  model: &#x27;orca-mini-3b-gguf2-q4_0&#x27;,
  modelPath: &#x27;./models/&#x27; // Optional: custom model directory
});

const agent = new Agent(
  {
    name: &#x27;gpt4all-agent&#x27;,
    model: &#x27;orca-mini-3b-gguf2-q4_0&#x27;,
    provider: &#x27;gpt4all&#x27;,
    systemPrompt: &#x27;You are a helpful assistant.&#x27;,
    tools: []
  },
  provider,
  new ToolRegistry()
);

const response = await agent.execute(&#x27;What is machine learning?&#x27;);
console.log(response.content);</code></pre>
                </div>
                <div class="border rounded-lg p-6 mb-6 bg-slate-50 shadow-sm">
                  <h3 class="text-2xl font-bold mb-3">
                    ü§ó HuggingFace Transformers
                  </h3>
                  <p class="mb-4">
                    Access thousands of open source models via HuggingFace
                    Inference API or self-hosted endpoints. Supports both cloud
                    and local deployment.
                  </p>
                  <h4 class="text-lg font-semibold mb-2">
                    Using Inference API (Free)
                  </h4>
                  <pre
                    class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4"
                  ><code>import { Agent, HuggingFaceProvider, ToolRegistry } from &#x27;@lov3kaizen/agentsea-core&#x27;;

// Free tier with rate limits
const provider = new HuggingFaceProvider({
  apiKey: process.env.HUGGINGFACE_API_KEY, // Get from hf.co
  model: &#x27;meta-llama/Meta-Llama-3-8B-Instruct&#x27;
});

const agent = new Agent(
  {
    name: &#x27;hf-agent&#x27;,
    model: &#x27;meta-llama/Meta-Llama-3-8B-Instruct&#x27;,
    provider: &#x27;huggingface&#x27;,
    systemPrompt: &#x27;You are a helpful AI assistant.&#x27;,
    tools: []
  },
  provider,
  new ToolRegistry()
);

const response = await agent.execute(&#x27;Explain neural networks&#x27;);
console.log(response.content);</code></pre>
                  <h4 class="text-lg font-semibold mb-2">
                    Self-Hosted with Text Generation Inference
                  </h4>
                  <pre
                    class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4"
                  ><code># Run TGI server locally
docker run -p 8080:80 \
  -v ./models:/data \
  ghcr.io/huggingface/text-generation-inference:latest \
  --model-id meta-llama/Meta-Llama-3-8B-Instruct

# Connect to local endpoint
const provider = new HuggingFaceProvider({
  baseUrl: &#x27;http://localhost:8080&#x27;,
  model: &#x27;meta-llama/Meta-Llama-3-8B-Instruct&#x27;
});</code></pre>
                  <h4 class="text-lg font-semibold mb-2">
                    Popular Open Source Models
                  </h4>
                  <ul class="list-disc pl-6 mb-4 space-y-1">
                    <li>
                      <strong>meta-llama/Meta-Llama-3.1-8B-Instruct</strong> -
                      Meta&#x27;s latest
                    </li>
                    <li>
                      <strong>mistralai/Mistral-7B-Instruct-v0.3</strong> -
                      Efficient instruct model
                    </li>
                    <li>
                      <strong>google/gemma-2-9b-it</strong> - Google&#x27;s open
                      model
                    </li>
                    <li>
                      <strong>Qwen/Qwen2.5-7B-Instruct</strong> - Multilingual
                      support
                    </li>
                    <li>
                      <strong>microsoft/Phi-3-mini-4k-instruct</strong> - Small
                      but capable (3.8B)
                    </li>
                  </ul>
                </div>
                <div class="border rounded-lg p-6 mb-6 bg-slate-50 shadow-sm">
                  <h3 class="text-2xl font-bold mb-3">üíª LM Studio</h3>
                  <p class="mb-4">
                    User-friendly desktop app for running local LLMs with a
                    beautiful UI. Includes model discovery, automatic
                    quantization selection, and OpenAI-compatible API server.
                  </p>
                  <h4 class="text-lg font-semibold mb-2">Setup</h4>
                  <ol class="list-decimal pl-6 mb-4 space-y-2">
                    <li>
                      Download LM Studio from
                      <a
                        href="https://lmstudio.ai"
                        class="text-blue-600 hover:underline"
                        >lmstudio.ai</a
                      >
                    </li>
                    <li>Browse and download models from the UI</li>
                    <li>Start the local server (OpenAI-compatible endpoint)</li>
                    <li>Connect AgentSea to the server</li>
                  </ol>
                  <h4 class="text-lg font-semibold mb-2">Basic Usage</h4>
                  <pre
                    class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4"
                  ><code>import { Agent, OpenAIProvider, ToolRegistry } from &#x27;@lov3kaizen/agentsea-core&#x27;;

// LM Studio uses OpenAI-compatible API
const provider = new OpenAIProvider({
  baseUrl: &#x27;http://localhost:1234/v1&#x27;,
  apiKey: &#x27;lm-studio&#x27;, // Any value works for local
  model: &#x27;local-model&#x27;
});

const agent = new Agent(
  {
    name: &#x27;lm-studio-agent&#x27;,
    model: &#x27;local-model&#x27;,
    provider: &#x27;openai&#x27;, // Uses OpenAI interface
    systemPrompt: &#x27;You are a helpful assistant.&#x27;,
    tools: []
  },
  provider,
  new ToolRegistry()
);

const response = await agent.execute(&#x27;Hello!&#x27;);
console.log(response.content);</code></pre>
                </div>
                <div class="border rounded-lg p-6 mb-6 bg-slate-50 shadow-sm">
                  <h3 class="text-2xl font-bold mb-3">
                    üå™Ô∏è Mistral AI (Self-Hosted)
                  </h3>
                  <p class="mb-4">
                    Mistral offers open-weight models that can be self-hosted.
                    Use their official Docker images or deploy via vLLM for
                    production workloads.
                  </p>
                  <h4 class="text-lg font-semibold mb-2">
                    Using vLLM (Recommended for Production)
                  </h4>
                  <pre
                    class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4"
                  ><code># Deploy Mistral 7B with vLLM
docker run -p 8000:8000 \
  --gpus all \
  vllm/vllm-openai:latest \
  --model mistralai/Mistral-7B-Instruct-v0.3 \
  --dtype float16

# Connect AgentSea agent
const provider = new OpenAIProvider({
  baseUrl: &#x27;http://localhost:8000/v1&#x27;,
  apiKey: &#x27;none&#x27;,
  model: &#x27;mistralai/Mistral-7B-Instruct-v0.3&#x27;
});</code></pre>
                </div>
                <h2 class="text-3xl font-bold mt-12 mb-4">
                  Provider Comparison
                </h2>
                <div class="overflow-x-auto mb-8">
                  <table
                    class="min-w-full bg-whblue-700 border border-gray-300"
                  >
                    <thead class="bg-gray-50">
                      <tr>
                        <th
                          class="px-6 py-3 border-b text-left text-sm font-semibold"
                        >
                          Provider
                        </th>
                        <th
                          class="px-6 py-3 border-b text-left text-sm font-semibold"
                        >
                          Ease of Use
                        </th>
                        <th
                          class="px-6 py-3 border-b text-left text-sm font-semibold"
                        >
                          Performance
                        </th>
                        <th
                          class="px-6 py-3 border-b text-left text-sm font-semibold"
                        >
                          GPU Required
                        </th>
                        <th
                          class="px-6 py-3 border-b text-left text-sm font-semibold"
                        >
                          Best For
                        </th>
                      </tr>
                    </thead>
                    <tbody class="divide-y divide-gray-200">
                      <tr>
                        <td class="px-6 py-4 text-sm">Ollama</td>
                        <td class="px-6 py-4 text-sm">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                        <td class="px-6 py-4 text-sm">‚≠ê‚≠ê‚≠ê‚≠ê</td>
                        <td class="px-6 py-4 text-sm">Optional</td>
                        <td class="px-6 py-4 text-sm">
                          Quick start, development
                        </td>
                      </tr>
                      <tr class="bg-gray-50">
                        <td class="px-6 py-4 text-sm">llama.cpp</td>
                        <td class="px-6 py-4 text-sm">‚≠ê‚≠ê‚≠ê</td>
                        <td class="px-6 py-4 text-sm">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                        <td class="px-6 py-4 text-sm">Optional</td>
                        <td class="px-6 py-4 text-sm">Maximum performance</td>
                      </tr>
                      <tr>
                        <td class="px-6 py-4 text-sm">GPT4All</td>
                        <td class="px-6 py-4 text-sm">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                        <td class="px-6 py-4 text-sm">‚≠ê‚≠ê‚≠ê</td>
                        <td class="px-6 py-4 text-sm">No</td>
                        <td class="px-6 py-4 text-sm">
                          Beginners, desktop apps
                        </td>
                      </tr>
                      <tr class="bg-gray-50">
                        <td class="px-6 py-4 text-sm">LM Studio</td>
                        <td class="px-6 py-4 text-sm">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                        <td class="px-6 py-4 text-sm">‚≠ê‚≠ê‚≠ê‚≠ê</td>
                        <td class="px-6 py-4 text-sm">Optional</td>
                        <td class="px-6 py-4 text-sm">
                          GUI users, testing models
                        </td>
                      </tr>
                      <tr>
                        <td class="px-6 py-4 text-sm">HuggingFace TGI</td>
                        <td class="px-6 py-4 text-sm">‚≠ê‚≠ê‚≠ê</td>
                        <td class="px-6 py-4 text-sm">‚≠ê‚≠ê‚≠ê‚≠ê</td>
                        <td class="px-6 py-4 text-sm">Recommended</td>
                        <td class="px-6 py-4 text-sm">Model variety</td>
                      </tr>
                      <tr class="bg-gray-50">
                        <td class="px-6 py-4 text-sm">vLLM</td>
                        <td class="px-6 py-4 text-sm">‚≠ê‚≠ê</td>
                        <td class="px-6 py-4 text-sm">‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê</td>
                        <td class="px-6 py-4 text-sm">Yes</td>
                        <td class="px-6 py-4 text-sm">
                          Production, high throughput
                        </td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <h2 class="text-3xl font-bold mt-12 mb-4">
                  Tool Calling with Local Models
                </h2>
                <p class="mb-4">
                  Many local models don&#x27;t natively support function calling
                  like Claude or GPT-4. AgentSea provides automatic tool calling
                  fallback using prompt engineering:
                </p>
                <pre
                  class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-6"
                ><code>import { Agent, OllamaProvider, ToolRegistry, Calculator, HttpRequest } from &#x27;@lov3kaizen/agentsea-core&#x27;;

const toolRegistry = new ToolRegistry();
toolRegistry.register(new Calculator());
toolRegistry.register(new HttpRequest());

const provider = new OllamaProvider({
  baseUrl: &#x27;http://localhost:11434&#x27;,
  model: &#x27;llama3.2&#x27;,
  // Enable tool calling adapter for models without native support
  useToolAdapter: true
});

const agent = new Agent(
  {
    name: &#x27;tool-agent&#x27;,
    model: &#x27;llama3.2&#x27;,
    provider: &#x27;ollama&#x27;,
    systemPrompt: &#x27;You are a helpful assistant with access to tools.&#x27;,
    tools: [
      { name: &#x27;calculator&#x27;, description: &#x27;Perform mathematical calculations&#x27; },
      { name: &#x27;http_request&#x27;, description: &#x27;Make HTTP requests to APIs&#x27; }
    ]
  },
  provider,
  toolRegistry
);

// Agent will automatically format tool calls in prompts
const response = await agent.execute(
  &#x27;What is 47 * 89 + 123?&#x27;
);

console.log(response.content);
// Uses calculator tool automatically</code></pre>
                <h2 class="text-3xl font-bold mt-12 mb-4">
                  Hardware Requirements
                </h2>
                <div class="bg-yellow-50 border-l-4 border-yellow-500 p-6 mb-6">
                  <h3 class="text-lg font-semibold mb-2">
                    Minimum Requirements
                  </h3>
                  <ul class="space-y-2">
                    <li>
                      <strong>CPU Only:</strong> 8GB RAM, modern CPU (3B models)
                    </li>
                    <li>
                      <strong>GPU Recommended:</strong> 16GB RAM, NVIDIA GPU
                      with 6GB+ VRAM (7B models)
                    </li>
                    <li>
                      <strong>Optimal:</strong> 32GB RAM, NVIDIA GPU with 12GB+
                      VRAM (13B+ models)
                    </li>
                  </ul>
                </div>
                <div class="overflow-x-auto mb-8">
                  <table
                    class="min-w-full bg-whblue-700 border border-gray-300"
                  >
                    <thead class="bg-gray-50">
                      <tr>
                        <th
                          class="px-6 py-3 border-b text-left text-sm font-semibold"
                        >
                          Model Size
                        </th>
                        <th
                          class="px-6 py-3 border-b text-left text-sm font-semibold"
                        >
                          RAM (CPU)
                        </th>
                        <th
                          class="px-6 py-3 border-b text-left text-sm font-semibold"
                        >
                          VRAM (GPU)
                        </th>
                        <th
                          class="px-6 py-3 border-b text-left text-sm font-semibold"
                        >
                          Performance
                        </th>
                      </tr>
                    </thead>
                    <tbody class="divide-y divide-gray-200">
                      <tr>
                        <td class="px-6 py-4 text-sm">3B (Q4)</td>
                        <td class="px-6 py-4 text-sm">4GB</td>
                        <td class="px-6 py-4 text-sm">3GB</td>
                        <td class="px-6 py-4 text-sm">Fast, basic tasks</td>
                      </tr>
                      <tr class="bg-gray-50">
                        <td class="px-6 py-4 text-sm">7B (Q4)</td>
                        <td class="px-6 py-4 text-sm">8GB</td>
                        <td class="px-6 py-4 text-sm">6GB</td>
                        <td class="px-6 py-4 text-sm">Good balance</td>
                      </tr>
                      <tr>
                        <td class="px-6 py-4 text-sm">13B (Q4)</td>
                        <td class="px-6 py-4 text-sm">16GB</td>
                        <td class="px-6 py-4 text-sm">10GB</td>
                        <td class="px-6 py-4 text-sm">High quality</td>
                      </tr>
                      <tr class="bg-gray-50">
                        <td class="px-6 py-4 text-sm">70B (Q4)</td>
                        <td class="px-6 py-4 text-sm">48GB</td>
                        <td class="px-6 py-4 text-sm">40GB</td>
                        <td class="px-6 py-4 text-sm">Near GPT-4 level</td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <h2 class="text-3xl font-bold mt-12 mb-4">Troubleshooting</h2>
                <div class="space-y-6">
                  <div class="border-l-4 border-red-500 pl-4">
                    <h3 class="font-bold mb-2">
                      Connection Refused / Cannot Connect
                    </h3>
                    <ul class="list-disc pl-6 space-y-1">
                      <li>
                        Verify the server is running:
                        <code class="bg-gray-100 px-2 py-1 rounded"
                          >curl http://localhost:11434</code
                        >
                      </li>
                      <li>Check firewall settings</li>
                      <li>Ensure correct port in baseUrl</li>
                    </ul>
                  </div>
                  <div class="border-l-4 border-red-500 pl-4">
                    <h3 class="font-bold mb-2">Out of Memory (OOM)</h3>
                    <ul class="list-disc pl-6 space-y-1">
                      <li>Use a smaller model (3B instead of 7B)</li>
                      <li>Try higher quantization (Q4 instead of Q8)</li>
                      <li>Reduce context length (maxTokens)</li>
                      <li>Enable GPU offloading if available</li>
                    </ul>
                  </div>
                  <div class="border-l-4 border-red-500 pl-4">
                    <h3 class="font-bold mb-2">Slow Response Times</h3>
                    <ul class="list-disc pl-6 space-y-1">
                      <li>Enable GPU acceleration (Metal/CUDA)</li>
                      <li>Use quantized models (Q4_K_M recommended)</li>
                      <li>Reduce batch size or context length</li>
                      <li>
                        Consider switching to llama.cpp for faster inference
                      </li>
                    </ul>
                  </div>
                  <div class="border-l-4 border-red-500 pl-4">
                    <h3 class="font-bold mb-2">Tool Calling Not Working</h3>
                    <ul class="list-disc pl-6 space-y-1">
                      <li>
                        Enable
                        <code class="bg-gray-100 px-2 py-1 rounded"
                          >useToolAdapter: true</code
                        >
                        in provider config
                      </li>
                      <li>Use models fine-tuned for instruction following</li>
                      <li>Check system prompt includes tool descriptions</li>
                      <li>
                        Consider using cloud providers for complex tool use
                      </li>
                    </ul>
                  </div>
                </div>
                <h2 class="text-3xl font-bold mt-12 mb-4">
                  Performance Optimization Tips
                </h2>
                <div class="grid md:grid-cols-2 gap-6 mb-8">
                  <div class="border rounded-lg p-6 bg-whblue-700 shadow-sm">
                    <h3 class="font-bold text-lg mb-3">
                      üöÄ Speed Optimization
                    </h3>
                    <ul class="space-y-2 text-sm">
                      <li>‚úì Use Q4_K_M quantization (best speed/quality)</li>
                      <li>
                        ‚úì Enable GPU layers (<code>--n-gpu-layers 35</code>)
                      </li>
                      <li>‚úì Increase batch size for throughput</li>
                      <li>‚úì Use smaller models (3B-7B) for simple tasks</li>
                      <li>‚úì Enable flash attention if available</li>
                    </ul>
                  </div>
                  <div class="border rounded-lg p-6 bg-whblue-700 shadow-sm">
                    <h3 class="font-bold text-lg mb-3">
                      üéØ Quality Optimization
                    </h3>
                    <ul class="space-y-2 text-sm">
                      <li>‚úì Use Q5_K_M or Q8 quantization</li>
                      <li>‚úì Choose larger models (13B-70B)</li>
                      <li>
                        ‚úì Adjust temperature (0.7 for creative, 0.1 for factual)
                      </li>
                      <li>‚úì Use instruct-tuned model variants</li>
                      <li>‚úì Provide clear system prompts</li>
                    </ul>
                  </div>
                </div>
                <h2 class="text-3xl font-bold mt-12 mb-4">
                  Production Deployment
                </h2>
                <pre
                  class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-6"
                ><code>version: &#x27;3.8&#x27;

services:
  # High-performance inference with vLLM
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - &quot;8000:8000&quot;
    volumes:
      - ./models:/root/.cache/huggingface
    environment:
      - CUDA_VISIBLE_DEVICES=0
    command: &gt;
      --model mistralai/Mistral-7B-Instruct-v0.3
      --dtype float16
      --max-model-len 4096
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Your AgentSea application
  agentsea-app:
    build: .
    ports:
      - &quot;3000:3000&quot;
    environment:
      - LOCAL_LLM_URL=http://vllm:8000
    depends_on:
      - vllm</code></pre>
                <div class="bg-green-50 border-l-4 border-green-500 p-6 mb-8">
                  <h3 class="text-lg font-semibold mb-2">
                    Ready to Get Started?
                  </h3>
                  <p class="mb-4">
                    Start with Ollama for the easiest setup, then explore other
                    providers based on your needs. All local providers work
                    seamlessly with AgentSea&#x27;s agent framework, tools, and
                    workflows.
                  </p>
                  <div class="space-x-4">
                    <a
                      href="/docs/quick-start"
                      class="text-blue-600 hover:underline font-semibold"
                      >Quick Start Guide ‚Üí</a
                    ><a
                      href="/docs/agents"
                      class="text-blue-600 hover:underline font-semibold"
                      >Agent Documentation ‚Üí</a
                    ><a
                      href="/examples"
                      class="text-blue-600 hover:underline font-semibold"
                      >See Examples ‚Üí</a
                    >
                  </div>
                </div>
                <h2 class="text-3xl font-bold mt-12 mb-4">Next Steps</h2>
                <ul class="list-disc pl-6 space-y-2 mb-8">
                  <li>
                    <a href="/docs/tools" class="text-blue-600 hover:underline"
                      >Learn about Tool Integration</a
                    >
                  </li>
                  <li>
                    <a
                      href="/docs/workflows"
                      class="text-blue-600 hover:underline"
                      >Build Multi-Agent Workflows</a
                    >
                  </li>
                  <li>
                    <a href="/docs/memory" class="text-blue-600 hover:underline"
                      >Configure Memory Stores</a
                    >
                  </li>
                  <li>
                    <a
                      href="/docs/observability"
                      class="text-blue-600 hover:underline"
                      >Monitor Performance</a
                    >
                  </li>
                </ul>
              </div>
            </div>
          </div>
          <!--$--><!--/$-->
        </main>
        <footer class="footer footer-center bg-base-200 text-base-content p-10">
          <nav class="grid grid-flow-col gap-4">
            <a class="link link-hover" href="/docs/voice/">Voice Features</a
            ><a class="link link-hover" href="/docs/local-models/"
              >Local Models</a
            ><a class="link link-hover" href="/docs/cli/">CLI Tool</a
            ><a class="link link-hover" href="/api/">REST API</a>
          </nav>
          <nav class="grid grid-flow-col gap-4">
            <a
              href="https://github.com/lovekaizen/agentsea"
              class="link link-hover"
              >GitHub</a
            ><a
              href="https://github.com/lovekaizen/agentsea/discussions"
              class="link link-hover"
              >Discussions</a
            ><a
              href="https://github.com/lovekaizen/agentsea/issues"
              class="link link-hover"
              >Issues</a
            >
          </nav>
          <aside><p>¬© 2025 AgentSea ADK. MIT License.</p></aside>
        </footer>
      </div>
      <div class="drawer-side border border-r-1 border-y-0 border-l-0">
        <label for="main-drawer" class="drawer-overlay"></label>
        <aside class="bg-base-200 min-h-full w-80">
          <ul class="menu p-4">
            <li>
              <h2 class="menu-title">Getting Started</h2>
              <ul>
                <li class="pl-2">
                  <a class="active:text-white" href="/">Home</a>
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/">Documentation</a>
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/installation/"
                    >Installation</a
                  >
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/quick-start/"
                    >Quick Start</a
                  >
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/cli/">CLI Tool</a>
                </li>
              </ul>
            </li>
            <li>
              <h2 class="menu-title">Core Concepts</h2>
              <ul>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/agents/">Agents</a>
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/providers/"
                    >Providers</a
                  >
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/local-providers/"
                    >Local Providers</a
                  >
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/tools/">Tools</a>
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/workflows/"
                    >Workflows</a
                  >
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/memory/">Memory</a>
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/formatting/"
                    >Content Formatting</a
                  >
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/conversation/"
                    >Conversation</a
                  >
                </li>
              </ul>
            </li>
            <li>
              <h2 class="menu-title">Features</h2>
              <ul>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/voice/"
                    >Voice (TTS/STT)</a
                  >
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/local-models/"
                    >Local Models</a
                  >
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/acp-integration/"
                    >ACP Commerce</a
                  >
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/api/"
                    >REST API &amp; Streaming</a
                  >
                </li>
              </ul>
            </li>
            <li>
              <h2 class="menu-title">MCP Integration</h2>
              <ul>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/mcp-overview/"
                    >Overview</a
                  >
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/mcp-servers/"
                    >MCP Servers</a
                  >
                </li>
              </ul>
            </li>
            <li>
              <h2 class="menu-title">Advanced</h2>
              <ul>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/multi-tenancy/"
                    >Multi-Tenancy</a
                  >
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/observability/"
                    >Observability</a
                  >
                </li>
                <li class="pl-2">
                  <a class="active:text-white" href="/docs/nestjs/"
                    >NestJS Integration</a
                  >
                </li>
              </ul>
            </li>
            <li>
              <h2 class="menu-title">Resources</h2>
              <ul>
                <li class="pl-2">
                  <a class="active:text-white" href="/examples/">Examples</a>
                </li>
              </ul>
            </li>
          </ul>
        </aside>
      </div>
    </div>
    <script
      src="/_next/static/chunks/webpack-1b7e2f9af6e0232d.js"
      id="_R_"
      async=""
    ></script>
    <script>
      (self.__next_f = self.__next_f || []).push([0]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '1:"$Sreact.fragment"\n2:I[1275,["619","static/chunks/619-ba102abea3e3d0e4.js","543","static/chunks/543-a0960cd7f2b1c22c.js","177","static/chunks/app/layout-b0f860a3d9150669.js"],"default"]\n3:I[9766,[],""]\n4:I[8924,[],""]\n23:I[7150,[],""]\n:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]\n:HL["/_next/static/css/7e7d96b1e6991756.css","style"]\n:HL["/_next/static/css/1145ad3a91ffb5c9.css","style"]\n:HL["/_next/static/css/7a622664f4c377b7.css","style"]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '0:{"P":null,"b":"BR2GXXFimfBinRJoMr3YY","p":"","c":["","docs","local-providers",""],"i":false,"f":[[["",{"children":["docs",{"children":["local-providers",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/7e7d96b1e6991756.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/1145ad3a91ffb5c9.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","2",{"rel":"stylesheet","href":"/_next/static/css/7a622664f4c377b7.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","data-theme":"cmyk","children":["$","body",null,{"className":"__className_f367f3","children":["$","$L2",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\\"Segoe UI\\",Roboto,Helvetica,Arial,sans-serif,\\"Apple Color Emoji\\",\\"Segoe UI Emoji\\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]}]]}],{"children":["docs",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["local-providers",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"min-h-screen bg-base-100","children":["$","div",null,{"className":"container mx-auto px-6 py-12 max-w-5xl ","children":[["$","div",null,{"className":"mb-12","children":[["$","h1",null,{"className":"text-5xl font-bold mb-4","children":"Local \u0026 Open Source Providers"}],["$","p",null,{"className":"text-xl text-base-content/70","children":"Run AI agents completely locally or with open source models. Perfect for privacy-sensitive applications, offline deployments, cost optimization, and development."}]]}],["$","div",null,{"className":"prose prose-lg max-w-none","children":[["$","div",null,{"className":"bg-slate-50 border-l-4 border-blue-500 p-6 mb-8","children":[["$","h3",null,{"className":"text-lg font-semibold mb-2","children":"Why Use Local Providers?"}],["$","ul",null,{"className":"space-y-2","children":[["$","li",null,{"children":["üîí ",["$","strong",null,{"children":"Privacy \u0026 Security"}]," - Data never leaves your infrastructure"]}],["$","li",null,{"children":["üí∞ ",["$","strong",null,{"children":"Cost Savings"}]," - No per-token API costs"]}],["$","li",null,{"children":["‚ö° ",["$","strong",null,{"children":"Low Latency"}]," - No network round trips for inference"]}],["$","li",null,{"children":["üîå ",["$","strong",null,{"children":"Offline Capable"}]," - Works without internet connection"]}],["$","li",null,{"children":["üéõÔ∏è ",["$","strong",null,{"children":"Full Control"}]," - Customize models, parameters, and deployment"]}]]}]]}],["$","h2",null,{"className":"text-3xl font-bold mt-12 mb-4","children":"Supported Local Providers"}],["$","div",null,{"className":"border rounded-lg p-6 mb-6 bg-slate-50 shadow-sm","children":[["$","h3",null,{"className":"text-2xl font-bold mb-3","children":"ü¶ô Ollama"}],["$","p",null,{"className":"mb-4","children":"The easiest way to run local LLMs. Supports Llama 3, Mistral, Gemma, and 50+ other models with automatic model management and GPU acceleration."}],"$L5","$L6","$L7","$L8","$L9","$La"]}],"$Lb","$Lc","$Ld","$Le","$Lf","$L10","$L11","$L12","$L13","$L14","$L15","$L16","$L17","$L18","$L19","$L1a","$L1b","$L1c","$L1d","$L1e","$L1f","$L20"]}]]}]}],null,"$L21"]}],{},null,false]},null,false]},null,false]},null,false],"$L22",false]],"m":"$undefined","G":["$23",[]],"s":false,"S":true}\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '24:I[4431,[],"OutletBoundary"]\n26:I[5278,[],"AsyncMetadataOutlet"]\n28:I[4431,[],"ViewportBoundary"]\n2a:I[4431,[],"MetadataBoundary"]\n2b:"$Sreact.suspense"\n5:["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Installation"}]\n6:["$","pre",null,{"className":"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4","children":["$","code",null,{"children":"# macOS/Linux\\ncurl -fsSL https://ollama.com/install.sh | sh\\n\\n# Windows\\n# Download from https://ollama.com/download\\n\\n# Pull a model\\nollama pull llama3.2\\nollama pull mistral\\nollama pull gemma2"}]}]\n7:["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Basic Usage"}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        "8:[\"$\",\"pre\",null,{\"className\":\"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4\",\"children\":[\"$\",\"code\",null,{\"children\":\"import { Agent, OllamaProvider, ToolRegistry } from '@lov3kaizen/agentsea-core';\\n\\n// Initialize Ollama provider\\nconst provider = new OllamaProvider({\\n  baseUrl: 'http://localhost:11434',\\n  model: 'llama3.2' // or 'mistral', 'gemma2', etc.\\n});\\n\\n// Create agent with local model\\nconst agent = new Agent(\\n  {\\n    name: 'local-assistant',\\n    model: 'llama3.2',\\n    provider: 'ollama',\\n    systemPrompt: 'You are a helpful AI assistant running locally.',\\n    tools: [],\\n    temperature: 0.7\\n  },\\n  provider,\\n  new ToolRegistry()\\n);\\n\\n// Execute locally\\nconst response = await agent.execute(\\n  'What are the benefits of running AI models locally?'\\n);\\n\\nconsole.log(response.content);\"}]}]\n",
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '9:["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Recommended Models"}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        'a:["$","ul",null,{"className":"list-disc pl-6 mb-4 space-y-2","children":[["$","li",null,{"children":[["$","strong",null,{"children":"llama3.2:3b"}]," - Fast, efficient, great for chat (3GB RAM)"]}],["$","li",null,{"children":[["$","strong",null,{"children":"llama3.1:8b"}]," - Balanced performance and quality (8GB RAM)"]}],["$","li",null,{"children":[["$","strong",null,{"children":"mistral:7b"}]," - Excellent instruction following (7GB RAM)"]}],["$","li",null,{"children":[["$","strong",null,{"children":"gemma2:9b"}]," - Google\'s efficient model (9GB RAM)"]}],["$","li",null,{"children":[["$","strong",null,{"children":"qwen2.5:7b"}]," - Strong multilingual support (7GB RAM)"]}]]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        'b:["$","div",null,{"className":"border rounded-lg p-6 mb-6 bg-slate-50 shadow-sm","children":[["$","h3",null,{"className":"text-2xl font-bold mb-3","children":"üöÄ llama.cpp"}],["$","p",null,{"className":"mb-4","children":"High-performance C++ inference engine with Metal (Mac), CUDA (NVIDIA), and CPU support. Provides the fastest local inference with quantized models."}],["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Installation"}],["$","pre",null,{"className":"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4","children":["$","code",null,{"children":"# Build from source\\ngit clone https://github.com/ggerganov/llama.cpp\\ncd llama.cpp\\n\\n# macOS (with Metal acceleration)\\nmake LLAMA_METAL=1\\n\\n# Linux with CUDA\\nmake LLAMA_CUDA=1\\n\\n# Start server\\n./llama-server \\\\\\n  -m models/llama-3.2-3b-q4_k_m.gguf \\\\\\n  --port 8080 \\\\\\n  --n-gpu-layers 35"}]}],["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Basic Usage"}],["$","pre",null,{"className":"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4","children":["$","code",null,{"children":"import { Agent, LlamaCppProvider, ToolRegistry } from \'@lov3kaizen/agentsea-core\';\\n\\n// Initialize llama.cpp provider\\nconst provider = new LlamaCppProvider({\\n  baseUrl: \'http://localhost:8080\',\\n  model: \'llama-3.2-3b-q4_k_m\'\\n});\\n\\nconst agent = new Agent(\\n  {\\n    name: \'fast-agent\',\\n    model: \'llama-3.2-3b-q4_k_m\',\\n    provider: \'llama-cpp\',\\n    systemPrompt: \'You are a fast, efficient AI assistant.\',\\n    tools: [],\\n    temperature: 0.7,\\n    maxTokens: 2048\\n  },\\n  provider,\\n  new ToolRegistry()\\n);\\n\\n// Stream responses for real-time output\\nconst stream = await agent.stream(\'Explain quantum computing\');\\n\\nfor await (const chunk of stream) {\\n  if (chunk.type === \'content\') {\\n    process.stdout.write(chunk.content);\\n  }\\n}"}]}],["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Download Quantized Models"}],["$","p",null,{"className":"mb-2","children":"Get pre-quantized GGUF models from HuggingFace:"}],["$","ul",null,{"className":"list-disc pl-6 mb-4 space-y-1","children":[["$","li",null,{"children":[["$","a",null,{"href":"https://huggingface.co/bartowski","className":"text-blue-600 hover:underline","children":"bartowski\'s collection"}]," - Wide variety of quantized models"]}],["$","li",null,{"children":[["$","a",null,{"href":"https://huggingface.co/TheBloke","className":"text-blue-600 hover:underline","children":"TheBloke\'s collection"}]," - Popular GGUF conversions"]}],["$","li",null,{"children":[["$","a",null,{"href":"https://huggingface.co/QuantFactory","className":"text-blue-600 hover:underline","children":"QuantFactory"}]," - High-quality quantizations"]}]]}]]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        'c:["$","div",null,{"className":"border rounded-lg p-6 mb-6 bg-slate-50 shadow-sm","children":[["$","h3",null,{"className":"text-2xl font-bold mb-3","children":"üåê GPT4All"}],["$","p",null,{"className":"mb-4","children":"Privacy-focused local LLM platform with easy-to-use desktop app and Python/TypeScript bindings. Great for beginners and non-technical users."}],["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Installation"}],["$","pre",null,{"className":"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4","children":["$","code",null,{"children":"# Install GPT4All package\\nnpm install gpt4all\\n\\n# Or download desktop app\\n# https://gpt4all.io/"}]}],["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Basic Usage"}],["$","pre",null,{"className":"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4","children":["$","code",null,{"children":"import { Agent, GPT4AllProvider, ToolRegistry } from \'@lov3kaizen/agentsea-core\';\\n\\n// Initialize GPT4All provider\\nconst provider = new GPT4AllProvider({\\n  model: \'orca-mini-3b-gguf2-q4_0\',\\n  modelPath: \'./models/\' // Optional: custom model directory\\n});\\n\\nconst agent = new Agent(\\n  {\\n    name: \'gpt4all-agent\',\\n    model: \'orca-mini-3b-gguf2-q4_0\',\\n    provider: \'gpt4all\',\\n    systemPrompt: \'You are a helpful assistant.\',\\n    tools: []\\n  },\\n  provider,\\n  new ToolRegistry()\\n);\\n\\nconst response = await agent.execute(\'What is machine learning?\');\\nconsole.log(response.content);"}]}]]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        'd:["$","div",null,{"className":"border rounded-lg p-6 mb-6 bg-slate-50 shadow-sm","children":[["$","h3",null,{"className":"text-2xl font-bold mb-3","children":"ü§ó HuggingFace Transformers"}],["$","p",null,{"className":"mb-4","children":"Access thousands of open source models via HuggingFace Inference API or self-hosted endpoints. Supports both cloud and local deployment."}],["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Using Inference API (Free)"}],["$","pre",null,{"className":"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4","children":["$","code",null,{"children":"import { Agent, HuggingFaceProvider, ToolRegistry } from \'@lov3kaizen/agentsea-core\';\\n\\n// Free tier with rate limits\\nconst provider = new HuggingFaceProvider({\\n  apiKey: process.env.HUGGINGFACE_API_KEY, // Get from hf.co\\n  model: \'meta-llama/Meta-Llama-3-8B-Instruct\'\\n});\\n\\nconst agent = new Agent(\\n  {\\n    name: \'hf-agent\',\\n    model: \'meta-llama/Meta-Llama-3-8B-Instruct\',\\n    provider: \'huggingface\',\\n    systemPrompt: \'You are a helpful AI assistant.\',\\n    tools: []\\n  },\\n  provider,\\n  new ToolRegistry()\\n);\\n\\nconst response = await agent.execute(\'Explain neural networks\');\\nconsole.log(response.content);"}]}],["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Self-Hosted with Text Generation Inference"}],["$","pre",null,{"className":"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4","children":["$","code",null,{"children":"# Run TGI server locally\\ndocker run -p 8080:80 \\\\\\n  -v ./models:/data \\\\\\n  ghcr.io/huggingface/text-generation-inference:latest \\\\\\n  --model-id meta-llama/Meta-Llama-3-8B-Instruct\\n\\n# Connect to local endpoint\\nconst provider = new HuggingFaceProvider({\\n  baseUrl: \'http://localhost:8080\',\\n  model: \'meta-llama/Meta-Llama-3-8B-Instruct\'\\n});"}]}],["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Popular Open Source Models"}],["$","ul",null,{"className":"list-disc pl-6 mb-4 space-y-1","children":[["$","li",null,{"children":[["$","strong",null,{"children":"meta-llama/Meta-Llama-3.1-8B-Instruct"}]," - Meta\'s latest"]}],["$","li",null,{"children":[["$","strong",null,{"children":"mistralai/Mistral-7B-Instruct-v0.3"}]," - Efficient instruct model"]}],["$","li",null,{"children":[["$","strong",null,{"children":"google/gemma-2-9b-it"}]," - Google\'s open model"]}],["$","li",null,{"children":[["$","strong",null,{"children":"Qwen/Qwen2.5-7B-Instruct"}]," - Multilingual support"]}],["$","li",null,{"children":[["$","strong",null,{"children":"microsoft/Phi-3-mini-4k-instruct"}]," - Small but capable (3.8B)"]}]]}]]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        'e:["$","div",null,{"className":"border rounded-lg p-6 mb-6 bg-slate-50 shadow-sm","children":[["$","h3",null,{"className":"text-2xl font-bold mb-3","children":"üíª LM Studio"}],["$","p",null,{"className":"mb-4","children":"User-friendly desktop app for running local LLMs with a beautiful UI. Includes model discovery, automatic quantization selection, and OpenAI-compatible API server."}],["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Setup"}],["$","ol",null,{"className":"list-decimal pl-6 mb-4 space-y-2","children":[["$","li",null,{"children":["Download LM Studio from ",["$","a",null,{"href":"https://lmstudio.ai","className":"text-blue-600 hover:underline","children":"lmstudio.ai"}]]}],["$","li",null,{"children":"Browse and download models from the UI"}],["$","li",null,{"children":"Start the local server (OpenAI-compatible endpoint)"}],["$","li",null,{"children":"Connect AgentSea to the server"}]]}],["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Basic Usage"}],["$","pre",null,{"className":"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4","children":["$","code",null,{"children":"import { Agent, OpenAIProvider, ToolRegistry } from \'@lov3kaizen/agentsea-core\';\\n\\n// LM Studio uses OpenAI-compatible API\\nconst provider = new OpenAIProvider({\\n  baseUrl: \'http://localhost:1234/v1\',\\n  apiKey: \'lm-studio\', // Any value works for local\\n  model: \'local-model\'\\n});\\n\\nconst agent = new Agent(\\n  {\\n    name: \'lm-studio-agent\',\\n    model: \'local-model\',\\n    provider: \'openai\', // Uses OpenAI interface\\n    systemPrompt: \'You are a helpful assistant.\',\\n    tools: []\\n  },\\n  provider,\\n  new ToolRegistry()\\n);\\n\\nconst response = await agent.execute(\'Hello!\');\\nconsole.log(response.content);"}]}]]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        'f:["$","div",null,{"className":"border rounded-lg p-6 mb-6 bg-slate-50 shadow-sm","children":[["$","h3",null,{"className":"text-2xl font-bold mb-3","children":"üå™Ô∏è Mistral AI (Self-Hosted)"}],["$","p",null,{"className":"mb-4","children":"Mistral offers open-weight models that can be self-hosted. Use their official Docker images or deploy via vLLM for production workloads."}],["$","h4",null,{"className":"text-lg font-semibold mb-2","children":"Using vLLM (Recommended for Production)"}],["$","pre",null,{"className":"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-4","children":["$","code",null,{"children":"# Deploy Mistral 7B with vLLM\\ndocker run -p 8000:8000 \\\\\\n  --gpus all \\\\\\n  vllm/vllm-openai:latest \\\\\\n  --model mistralai/Mistral-7B-Instruct-v0.3 \\\\\\n  --dtype float16\\n\\n# Connect AgentSea agent\\nconst provider = new OpenAIProvider({\\n  baseUrl: \'http://localhost:8000/v1\',\\n  apiKey: \'none\',\\n  model: \'mistralai/Mistral-7B-Instruct-v0.3\'\\n});"}]}]]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '10:["$","h2",null,{"className":"text-3xl font-bold mt-12 mb-4","children":"Provider Comparison"}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '11:["$","div",null,{"className":"overflow-x-auto mb-8","children":["$","table",null,{"className":"min-w-full bg-whblue-700 border border-gray-300","children":[["$","thead",null,{"className":"bg-gray-50","children":["$","tr",null,{"children":[["$","th",null,{"className":"px-6 py-3 border-b text-left text-sm font-semibold","children":"Provider"}],["$","th",null,{"className":"px-6 py-3 border-b text-left text-sm font-semibold","children":"Ease of Use"}],["$","th",null,{"className":"px-6 py-3 border-b text-left text-sm font-semibold","children":"Performance"}],["$","th",null,{"className":"px-6 py-3 border-b text-left text-sm font-semibold","children":"GPU Required"}],["$","th",null,{"className":"px-6 py-3 border-b text-left text-sm font-semibold","children":"Best For"}]]}]}],["$","tbody",null,{"className":"divide-y divide-gray-200","children":[["$","tr",null,{"children":[["$","td",null,{"className":"px-6 py-4 text-sm","children":"Ollama"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Optional"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Quick start, development"}]]}],["$","tr",null,{"className":"bg-gray-50","children":[["$","td",null,{"className":"px-6 py-4 text-sm","children":"llama.cpp"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Optional"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Maximum performance"}]]}],["$","tr",null,{"children":[["$","td",null,{"className":"px-6 py-4 text-sm","children":"GPT4All"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"No"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Beginners, desktop apps"}]]}],["$","tr",null,{"className":"bg-gray-50","children":[["$","td",null,{"className":"px-6 py-4 text-sm","children":"LM Studio"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Optional"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"GUI users, testing models"}]]}],["$","tr",null,{"children":[["$","td",null,{"className":"px-6 py-4 text-sm","children":"HuggingFace TGI"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Recommended"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Model variety"}]]}],["$","tr",null,{"className":"bg-gray-50","children":[["$","td",null,{"className":"px-6 py-4 text-sm","children":"vLLM"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"‚≠ê‚≠ê"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Yes"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Production, high throughput"}]]}]]}]]}]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '12:["$","h2",null,{"className":"text-3xl font-bold mt-12 mb-4","children":"Tool Calling with Local Models"}]\n13:["$","p",null,{"className":"mb-4","children":"Many local models don\'t natively support function calling like Claude or GPT-4. AgentSea provides automatic tool calling fallback using prompt engineering:"}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        "14:[\"$\",\"pre\",null,{\"className\":\"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-6\",\"children\":[\"$\",\"code\",null,{\"children\":\"import { Agent, OllamaProvider, ToolRegistry, Calculator, HttpRequest } from '@lov3kaizen/agentsea-core';\\n\\nconst toolRegistry = new ToolRegistry();\\ntoolRegistry.register(new Calculator());\\ntoolRegistry.register(new HttpRequest());\\n\\nconst provider = new OllamaProvider({\\n  baseUrl: 'http://localhost:11434',\\n  model: 'llama3.2',\\n  // Enable tool calling adapter for models without native support\\n  useToolAdapter: true\\n});\\n\\nconst agent = new Agent(\\n  {\\n    name: 'tool-agent',\\n    model: 'llama3.2',\\n    provider: 'ollama',\\n    systemPrompt: 'You are a helpful assistant with access to tools.',\\n    tools: [\\n      { name: 'calculator', description: 'Perform mathematical calculations' },\\n      { name: 'http_request', description: 'Make HTTP requests to APIs' }\\n    ]\\n  },\\n  provider,\\n  toolRegistry\\n);\\n\\n// Agent will automatically format tool calls in prompts\\nconst response = await agent.execute(\\n  'What is 47 * 89 + 123?'\\n);\\n\\nconsole.log(response.content);\\n// Uses calculator tool automatically\"}]}]\n",
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '15:["$","h2",null,{"className":"text-3xl font-bold mt-12 mb-4","children":"Hardware Requirements"}]\n16:["$","div",null,{"className":"bg-yellow-50 border-l-4 border-yellow-500 p-6 mb-6","children":[["$","h3",null,{"className":"text-lg font-semibold mb-2","children":"Minimum Requirements"}],["$","ul",null,{"className":"space-y-2","children":[["$","li",null,{"children":[["$","strong",null,{"children":"CPU Only:"}]," 8GB RAM, modern CPU (3B models)"]}],["$","li",null,{"children":[["$","strong",null,{"children":"GPU Recommended:"}]," 16GB RAM, NVIDIA GPU with 6GB+ VRAM (7B models)"]}],["$","li",null,{"children":[["$","strong",null,{"children":"Optimal:"}]," 32GB RAM, NVIDIA GPU with 12GB+ VRAM (13B+ models)"]}]]}]]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '17:["$","div",null,{"className":"overflow-x-auto mb-8","children":["$","table",null,{"className":"min-w-full bg-whblue-700 border border-gray-300","children":[["$","thead",null,{"className":"bg-gray-50","children":["$","tr",null,{"children":[["$","th",null,{"className":"px-6 py-3 border-b text-left text-sm font-semibold","children":"Model Size"}],["$","th",null,{"className":"px-6 py-3 border-b text-left text-sm font-semibold","children":"RAM (CPU)"}],["$","th",null,{"className":"px-6 py-3 border-b text-left text-sm font-semibold","children":"VRAM (GPU)"}],["$","th",null,{"className":"px-6 py-3 border-b text-left text-sm font-semibold","children":"Performance"}]]}]}],["$","tbody",null,{"className":"divide-y divide-gray-200","children":[["$","tr",null,{"children":[["$","td",null,{"className":"px-6 py-4 text-sm","children":"3B (Q4)"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"4GB"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"3GB"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Fast, basic tasks"}]]}],["$","tr",null,{"className":"bg-gray-50","children":[["$","td",null,{"className":"px-6 py-4 text-sm","children":"7B (Q4)"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"8GB"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"6GB"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Good balance"}]]}],["$","tr",null,{"children":[["$","td",null,{"className":"px-6 py-4 text-sm","children":"13B (Q4)"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"16GB"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"10GB"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"High quality"}]]}],["$","tr",null,{"className":"bg-gray-50","children":[["$","td",null,{"className":"px-6 py-4 text-sm","children":"70B (Q4)"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"48GB"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"40GB"}],["$","td",null,{"className":"px-6 py-4 text-sm","children":"Near GPT-4 level"}]]}]]}]]}]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '18:["$","h2",null,{"className":"text-3xl font-bold mt-12 mb-4","children":"Troubleshooting"}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '19:["$","div",null,{"className":"space-y-6","children":[["$","div",null,{"className":"border-l-4 border-red-500 pl-4","children":[["$","h3",null,{"className":"font-bold mb-2","children":"Connection Refused / Cannot Connect"}],["$","ul",null,{"className":"list-disc pl-6 space-y-1","children":[["$","li",null,{"children":["Verify the server is running: ",["$","code",null,{"className":"bg-gray-100 px-2 py-1 rounded","children":"curl http://localhost:11434"}]]}],["$","li",null,{"children":"Check firewall settings"}],["$","li",null,{"children":"Ensure correct port in baseUrl"}]]}]]}],["$","div",null,{"className":"border-l-4 border-red-500 pl-4","children":[["$","h3",null,{"className":"font-bold mb-2","children":"Out of Memory (OOM)"}],["$","ul",null,{"className":"list-disc pl-6 space-y-1","children":[["$","li",null,{"children":"Use a smaller model (3B instead of 7B)"}],["$","li",null,{"children":"Try higher quantization (Q4 instead of Q8)"}],["$","li",null,{"children":"Reduce context length (maxTokens)"}],["$","li",null,{"children":"Enable GPU offloading if available"}]]}]]}],["$","div",null,{"className":"border-l-4 border-red-500 pl-4","children":[["$","h3",null,{"className":"font-bold mb-2","children":"Slow Response Times"}],["$","ul",null,{"className":"list-disc pl-6 space-y-1","children":[["$","li",null,{"children":"Enable GPU acceleration (Metal/CUDA)"}],["$","li",null,{"children":"Use quantized models (Q4_K_M recommended)"}],["$","li",null,{"children":"Reduce batch size or context length"}],["$","li",null,{"children":"Consider switching to llama.cpp for faster inference"}]]}]]}],["$","div",null,{"className":"border-l-4 border-red-500 pl-4","children":[["$","h3",null,{"className":"font-bold mb-2","children":"Tool Calling Not Working"}],["$","ul",null,{"className":"list-disc pl-6 space-y-1","children":[["$","li",null,{"children":["Enable ",["$","code",null,{"className":"bg-gray-100 px-2 py-1 rounded","children":"useToolAdapter: true"}]," in provider config"]}],["$","li",null,{"children":"Use models fine-tuned for instruction following"}],["$","li",null,{"children":"Check system prompt includes tool descriptions"}],["$","li",null,{"children":"Consider using cloud providers for complex tool use"}]]}]]}]]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '1a:["$","h2",null,{"className":"text-3xl font-bold mt-12 mb-4","children":"Performance Optimization Tips"}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '1b:["$","div",null,{"className":"grid md:grid-cols-2 gap-6 mb-8","children":[["$","div",null,{"className":"border rounded-lg p-6 bg-whblue-700 shadow-sm","children":[["$","h3",null,{"className":"font-bold text-lg mb-3","children":"üöÄ Speed Optimization"}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"children":"‚úì Use Q4_K_M quantization (best speed/quality)"}],["$","li",null,{"children":["‚úì Enable GPU layers (",["$","code",null,{"children":"--n-gpu-layers 35"}],")"]}],["$","li",null,{"children":"‚úì Increase batch size for throughput"}],["$","li",null,{"children":"‚úì Use smaller models (3B-7B) for simple tasks"}],["$","li",null,{"children":"‚úì Enable flash attention if available"}]]}]]}],["$","div",null,{"className":"border rounded-lg p-6 bg-whblue-700 shadow-sm","children":[["$","h3",null,{"className":"font-bold text-lg mb-3","children":"üéØ Quality Optimization"}],["$","ul",null,{"className":"space-y-2 text-sm","children":[["$","li",null,{"children":"‚úì Use Q5_K_M or Q8 quantization"}],["$","li",null,{"children":"‚úì Choose larger models (13B-70B)"}],["$","li",null,{"children":"‚úì Adjust temperature (0.7 for creative, 0.1 for factual)"}],["$","li",null,{"children":"‚úì Use instruct-tuned model variants"}],["$","li",null,{"children":"‚úì Provide clear system prompts"}]]}]]}]]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '1c:["$","h2",null,{"className":"text-3xl font-bold mt-12 mb-4","children":"Production Deployment"}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '1d:["$","pre",null,{"className":"bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto mb-6","children":["$","code",null,{"children":"version: \'3.8\'\\n\\nservices:\\n  # High-performance inference with vLLM\\n  vllm:\\n    image: vllm/vllm-openai:latest\\n    ports:\\n      - \\"8000:8000\\"\\n    volumes:\\n      - ./models:/root/.cache/huggingface\\n    environment:\\n      - CUDA_VISIBLE_DEVICES=0\\n    command: \u003e\\n      --model mistralai/Mistral-7B-Instruct-v0.3\\n      --dtype float16\\n      --max-model-len 4096\\n    deploy:\\n      resources:\\n        reservations:\\n          devices:\\n            - driver: nvidia\\n              count: 1\\n              capabilities: [gpu]\\n\\n  # Your AgentSea application\\n  agentsea-app:\\n    build: .\\n    ports:\\n      - \\"3000:3000\\"\\n    environment:\\n      - LOCAL_LLM_URL=http://vllm:8000\\n    depends_on:\\n      - vllm"}]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '1e:["$","div",null,{"className":"bg-green-50 border-l-4 border-green-500 p-6 mb-8","children":[["$","h3",null,{"className":"text-lg font-semibold mb-2","children":"Ready to Get Started?"}],["$","p",null,{"className":"mb-4","children":"Start with Ollama for the easiest setup, then explore other providers based on your needs. All local providers work seamlessly with AgentSea\'s agent framework, tools, and workflows."}],["$","div",null,{"className":"space-x-4","children":[["$","a",null,{"href":"/docs/quick-start","className":"text-blue-600 hover:underline font-semibold","children":"Quick Start Guide ‚Üí"}],["$","a",null,{"href":"/docs/agents","className":"text-blue-600 hover:underline font-semibold","children":"Agent Documentation ‚Üí"}],["$","a",null,{"href":"/examples","className":"text-blue-600 hover:underline font-semibold","children":"See Examples ‚Üí"}]]}]]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '1f:["$","h2",null,{"className":"text-3xl font-bold mt-12 mb-4","children":"Next Steps"}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '20:["$","ul",null,{"className":"list-disc pl-6 space-y-2 mb-8","children":[["$","li",null,{"children":["$","a",null,{"href":"/docs/tools","className":"text-blue-600 hover:underline","children":"Learn about Tool Integration"}]}],["$","li",null,{"children":["$","a",null,{"href":"/docs/workflows","className":"text-blue-600 hover:underline","children":"Build Multi-Agent Workflows"}]}],["$","li",null,{"children":["$","a",null,{"href":"/docs/memory","className":"text-blue-600 hover:underline","children":"Configure Memory Stores"}]}],["$","li",null,{"children":["$","a",null,{"href":"/docs/observability","className":"text-blue-600 hover:underline","children":"Monitor Performance"}]}]]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '21:["$","$L24",null,{"children":["$L25",["$","$L26",null,{"promise":"$@27"}]]}]\n22:["$","$1","h",{"children":[null,[["$","$L28",null,{"children":"$L29"}],["$","meta",null,{"name":"next-size-adjust","content":""}]],["$","$L2a",null,{"children":["$","div",null,{"hidden":true,"children":["$","$2b",null,{"fallback":null,"children":"$L2c"}]}]}]]}]\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '29:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]\n25:null\n',
      ]);
    </script>
    <script>
      self.__next_f.push([
        1,
        '27:{"metadata":[["$","title","0",{"children":"AgentSea - Unite and Orchestrate AI Agents"}],["$","meta","1",{"name":"description","content":"Build powerful agentic AI applications with AgentSea. Unite AI agents and services with multi-provider support, workflow orchestration, MCP protocol, and enterprise-grade observability."}]],"error":null,"digest":"$undefined"}\n',
      ]);
    </script>
    <script>
      self.__next_f.push([1, '2c:"$27:metadata"\n']);
    </script>
  </body>
</html>
