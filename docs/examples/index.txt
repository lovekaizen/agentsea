1:"$Sreact.fragment"
2:I[1275,["619","static/chunks/619-ba102abea3e3d0e4.js","543","static/chunks/543-a0960cd7f2b1c22c.js","177","static/chunks/app/layout-b0f860a3d9150669.js"],"default"]
3:I[9766,[],""]
4:I[8924,[],""]
5:I[1191,["619","static/chunks/619-ba102abea3e3d0e4.js","92","static/chunks/app/examples/page-339de7a5895506d2.js"],"default"]
a:I[7150,[],""]
:HL["/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/_next/static/css/7e7d96b1e6991756.css","style"]
:HL["/_next/static/css/1145ad3a91ffb5c9.css","style"]
:HL["/_next/static/css/7a622664f4c377b7.css","style"]
0:{"P":null,"b":"BR2GXXFimfBinRJoMr3YY","p":"","c":["","examples",""],"i":false,"f":[[["",{"children":["examples",{"children":["__PAGE__",{}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/7e7d96b1e6991756.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/_next/static/css/1145ad3a91ffb5c9.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}],["$","link","2",{"rel":"stylesheet","href":"/_next/static/css/7a622664f4c377b7.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","data-theme":"cmyk","children":["$","body",null,{"className":"__className_f367f3","children":["$","$L2",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}]}]]}],{"children":["examples",["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"min-h-screen bg-base-100","children":["$","div",null,{"className":"container mx-auto px-6 py-12 max-w-7xl ","children":[["$","div",null,{"className":"mb-12","children":[["$","h1",null,{"className":"text-5xl font-bold mb-4","children":"Examples"}],["$","p",null,{"className":"text-xl text-base-content/70","children":"Explore practical examples to learn how to build powerful agentic AI applications with AgentSea."}]]}],["$","$L5",null,{"examples":[{"icon":"üí¨","title":"Basic Chatbot","description":"Create a simple conversational agent with memory and tool calling capabilities.","tags":["Agent","Memory","Tools"],"category":"Getting Started","difficulty":"Beginner","code":"import {\n  Agent,\n  AnthropicProvider,\n  ToolRegistry,\n  BufferMemory,\n  calculatorTool,\n} from '@lov3kaizen/agentsea-core';\n\nconst agent = new Agent(\n  {\n    name: 'chatbot',\n    model: 'claude-sonnet-4-20250514',\n    provider: 'anthropic',\n    systemPrompt: 'You are a helpful assistant.',\n    tools: [calculatorTool],\n  },\n  new AnthropicProvider(),\n  new ToolRegistry(),\n  new BufferMemory(50),\n);\n\nconst response = await agent.execute(\n  'What is 42 * 58?',\n  { conversationId: 'user-123', sessionData: {}, history: [] }\n);","useCases":["Customer support chatbots","Virtual assistants","FAQ automation"]},{"icon":"üìù","title":"Content Pipeline","description":"Sequential workflow for research, writing, and editing content.","tags":["Workflow","Sequential","Multi-Agent"],"category":"Workflows","difficulty":"Intermediate","code":"import { WorkflowFactory } from '@lov3kaizen/agentsea-core';\n\nconst workflow = WorkflowFactory.create(\n  {\n    name: 'content-pipeline',\n    type: 'sequential',\n    agents: [\n      {\n        name: 'researcher',\n        systemPrompt: 'Research and gather information.',\n        tools: [httpRequestTool],\n      },\n      {\n        name: 'writer',\n        systemPrompt: 'Write comprehensive content.',\n      },\n      {\n        name: 'editor',\n        systemPrompt: 'Edit and polish for publication.',\n      },\n    ],\n  },\n  provider,\n  toolRegistry,\n);\n\nconst result = await workflow.execute(\n  'Write an article about AI agents',\n  context\n);","useCases":["Blog post generation","Report writing","Content marketing"]},{"icon":"üîå","title":"MCP Integration","description":"Connect to MCP servers for filesystem, GitHub, and more.","tags":["MCP","Tools","Integration"],"category":"Integration","difficulty":"Intermediate","code":"import { MCPRegistry } from '@lov3kaizen/agentsea-core';\n\nconst mcpRegistry = new MCPRegistry();\n\nawait mcpRegistry.addServer({\n  name: 'filesystem',\n  command: 'npx',\n  args: ['-y', '@modelcontextprotocol/server-filesystem', '/tmp'],\n  transport: 'stdio',\n});\n\nawait mcpRegistry.addServer({\n  name: 'github',\n  command: 'npx',\n  args: ['-y', '@modelcontextprotocol/server-github'],\n  transport: 'stdio',\n  env: { GITHUB_TOKEN: process.env.GITHUB_TOKEN },\n});\n\nconst tools = mcpRegistry.getTools();\n// Tools: filesystem:read_file, github:create_issue, etc.","useCases":["File system automation","GitHub operations","Database queries"]},{"icon":"üéØ","title":"Customer Support Router","description":"Supervisor workflow that routes requests to specialized agents.","tags":["Workflow","Supervisor","Routing"],"category":"Workflows","difficulty":"Advanced","code":"import { WorkflowFactory } from '@lov3kaizen/agentsea-core';\n\nconst workflow = WorkflowFactory.create(\n  {\n    name: 'support-router',\n    type: 'supervisor',\n    supervisor: {\n      name: 'router',\n      systemPrompt: 'Route to: technical-support, billing, or general',\n    },\n    agents: [\n      {\n        name: 'technical-support',\n        systemPrompt: 'Provide technical support.',\n        tools: [databaseQueryTool],\n      },\n      {\n        name: 'billing',\n        systemPrompt: 'Handle billing inquiries.',\n      },\n      {\n        name: 'general',\n        systemPrompt: 'General support.',\n      },\n    ],\n  },\n  provider,\n  toolRegistry,\n);","useCases":["Support ticket routing","Department-specific help","Escalation handling"]},{"icon":"üìä","title":"Data Analysis Pipeline","description":"Parallel workflow for multi-perspective data analysis.","tags":["Workflow","Parallel","Analysis"],"category":"Workflows","difficulty":"Intermediate","code":"import { WorkflowFactory } from '@lov3kaizen/agentsea-core';\n\nconst workflow = WorkflowFactory.create(\n  {\n    name: 'analysis',\n    type: 'parallel',\n    agents: [\n      {\n        name: 'sentiment',\n        systemPrompt: 'Analyze sentiment.',\n      },\n      {\n        name: 'keywords',\n        systemPrompt: 'Extract keywords.',\n      },\n      {\n        name: 'summary',\n        systemPrompt: 'Summarize content.',\n      },\n    ],\n  },\n  provider,\n  toolRegistry,\n);\n\nconst result = await workflow.execute(\n  'Analyze this product review: ...',\n  context\n);","useCases":["Product review analysis","Social media monitoring","Market research"]},{"icon":"üè¢","title":"NestJS Application","description":"Enterprise-ready agent service with NestJS integration.","tags":["NestJS","API","Production"],"category":"Integration","difficulty":"Advanced","code":"import { Module, Controller, Injectable } from '@nestjs/common';\nimport { AgenticModule } from '@lov3kaizen/agentsea-nestjs';\n\n@Module({\n  imports: [\n    AgenticModule.forRoot({\n      provider: new AnthropicProvider(),\n      defaultConfig: {\n        model: 'claude-sonnet-4-20250514',\n        provider: 'anthropic',\n      },\n    }),\n  ],\n})\nexport class AppModule {}\n\n@Injectable()\nexport class ChatService {\n  async chat(message: string) {\n    return this.agent.execute(message, context);\n  }\n}\n\n@Controller('chat')\nexport class ChatController {\n  @Post()\n  async chat(@Body('message') message: string) {\n    return this.chatService.chat(message);\n  }\n}","useCases":["REST API services","Enterprise applications","Microservices"]},{"icon":"ü¶ô","title":"Local Agent with Ollama","description":"Run agents completely offline with local models for privacy and cost savings.","tags":["Local","Ollama","Privacy"],"category":"Local Models","difficulty":"Beginner","code":"import {\n  Agent,\n  OllamaProvider,\n  ToolRegistry,\n  BufferMemory,\n  calculatorTool,\n} from '@lov3kaizen/agentsea-core';\n\n// No API key needed - runs locally!\nconst provider = new OllamaProvider({\n  baseUrl: 'http://localhost:11434',\n  model: 'llama3.2' // or mistral, gemma2, etc.\n});\n\nconst agent = new Agent(\n  {\n    name: 'local-assistant',\n    model: 'llama3.2',\n    provider: 'ollama',\n    systemPrompt: 'You are a helpful assistant running locally.',\n    tools: [calculatorTool],\n    temperature: 0.7,\n  },\n  provider,\n  new ToolRegistry(),\n  new BufferMemory(50),\n);\n\n// Completely private - no data leaves your machine\nconst response = await agent.execute(\n  'What is 156 * 89?',\n  { conversationId: 'local-user', sessionData: {}, history: [] }\n);","useCases":["Privacy-sensitive applications","Offline/air-gapped environments","Cost-free development"]},{"icon":"‚ö°","title":"High-Performance Local Workflow","description":"Multi-agent workflow using llama.cpp for maximum speed.","tags":["Local","llama.cpp","Workflow"],"category":"Local Models","difficulty":"Advanced","code":"import { WorkflowFactory, LlamaCppProvider } from '@lov3kaizen/agentsea-core';\n\n// Ultra-fast inference with llama.cpp\nconst provider = new LlamaCppProvider({\n  baseUrl: 'http://localhost:8080',\n  model: 'llama-3.2-3b-q4_k_m'\n});\n\nconst workflow = WorkflowFactory.create(\n  {\n    name: 'local-analysis',\n    type: 'parallel',\n    agents: [\n      {\n        name: 'summarizer',\n        model: 'llama-3.2-3b-q4_k_m',\n        provider: 'llama-cpp',\n        systemPrompt: 'Summarize the main points.',\n      },\n      {\n        name: 'sentiment',\n        model: 'llama-3.2-3b-q4_k_m',\n        provider: 'llama-cpp',\n        systemPrompt: 'Analyze sentiment.',\n      },\n      {\n        name: 'keywords',\n        model: 'llama-3.2-3b-q4_k_m',\n        provider: 'llama-cpp',\n        systemPrompt: 'Extract key terms.',\n      },\n    ],\n  },\n  provider,\n  new ToolRegistry(),\n);\n\n// All agents run in parallel locally - blazing fast!\nconst result = await workflow.execute(\n  'Analyze: The product is great but expensive.',\n  context\n);","useCases":["Real-time text analysis","High-throughput processing","Low-latency requirements"]},{"icon":"üìà","title":"Observability Setup","description":"Monitor agents with logging, metrics, and distributed tracing.","tags":["Observability","Monitoring","Metrics"],"category":"Production","difficulty":"Advanced","code":"import { Logger, globalMetrics, globalTracer } from '@lov3kaizen/agentsea-core';\n\nconst logger = new Logger({ level: 'info' });\n\n// Log execution\nlogger.info('Agent started', { agentName: 'chat-agent' });\n\n// Track metrics\nglobalMetrics.recordCounter('agent.executions', 1, {\n  agentName: 'chat-agent',\n  status: 'success',\n});\n\nglobalMetrics.recordHistogram('agent.latency', 1250, {\n  agentName: 'chat-agent',\n});\n\n// Create trace\nconst trace = globalTracer.createTrace('user-request');\nconst span = trace.createSpan('agent-execution');\n// ... execute agent\nspan.end();\n\n// Export to monitoring service\nglobalMetrics.subscribe((metric) => {\n  sendToPrometheus(metric);\n});","useCases":["Performance monitoring","Error tracking","Usage analytics"]},{"icon":"üîê","title":"Advanced Memory Management","description":"Use Redis and summary memory for persistent, scalable storage.","tags":["Memory","Redis","Production"],"category":"Production","difficulty":"Advanced","code":"import { RedisMemory, SummaryMemory } from '@lov3kaizen/agentsea-core';\n\n// Redis for persistence\nconst redisMemory = new RedisMemory({\n  url: 'redis://localhost:6379',\n  ttl: 86400, // 24 hours\n  keyPrefix: 'agent:',\n});\n\n// Summary for long conversations\nconst summaryMemory = new SummaryMemory(\n  new AnthropicProvider(),\n  {\n    maxMessages: 20,\n    summaryModel: 'claude-haiku-4-20250514',\n  },\n);\n\nconst agent = new Agent(\n  config,\n  provider,\n  toolRegistry,\n  redisMemory, // or summaryMemory\n);\n\n// Memory persists across restarts\nawait agent.execute('Remember: my name is Alice', context);","useCases":["Multi-server deployments","Long-running conversations","Cost optimization"]},{"icon":"üè¢","title":"Multi-Tenancy Support","description":"Build SaaS applications with complete tenant isolation and API key authentication.","tags":["Multi-Tenancy","SaaS","Enterprise"],"category":"Production","difficulty":"Advanced","code":"import {\n  TenantManager,\n  MemoryTenantStorage,\n  TenantBufferMemory,\n  Agent,\n} from '@lov3kaizen/agentsea-core';\n\n// Initialize multi-tenancy\nconst storage = new MemoryTenantStorage();\nconst tenantManager = new TenantManager(storage);\n\n// Create tenant\nconst tenant = await tenantManager.createTenant({\n  name: 'Acme Corp',\n  slug: 'acme-corp',\n  settings: {\n    maxAgents: 10,\n    maxConversations: 100,\n    allowedProviders: ['anthropic'],\n  },\n});\n\n// Generate API key\nconst apiKey = await tenantManager.generateApiKey(tenant.id);\n\n// Tenant-isolated memory\nconst memory = new TenantBufferMemory();\n\n// Execute with tenant context\nconst response = await agent.execute(message, {\n  conversationId: 'conv-1',\n  sessionData: { tenantId: tenant.id },\n  history: await memory.load(tenant.id, 'conv-1'),\n});\n\n// Track usage\nawait tenantManager.recordQuotaUsage(tenant.id, {\n  resource: 'api_calls',\n  amount: 1,\n  period: 'hourly',\n});","useCases":["Multi-tenant SaaS platforms","Enterprise customer isolation","Usage tracking and billing"]},{"icon":"üéôÔ∏è","title":"Voice-Enabled Agent","description":"Create agents with built-in speech-to-text and text-to-speech capabilities.","tags":["Voice","STT","TTS"],"category":"Integration","difficulty":"Intermediate","code":"import {\n  VoiceAgent,\n  Agent,\n  OpenAIWhisperProvider,\n  OpenAITTSProvider,\n  AnthropicProvider,\n  ToolRegistry,\n} from '@lov3kaizen/agentsea-core';\nimport fs from 'fs';\n\n// Create base agent\nconst agent = new Agent(\n  {\n    name: 'voice-assistant',\n    model: 'claude-sonnet-4-20250514',\n    provider: 'anthropic',\n    systemPrompt: 'You are a helpful voice assistant.',\n  },\n  new AnthropicProvider(),\n  new ToolRegistry(),\n);\n\n// Wrap with voice capabilities\nconst voiceAgent = new VoiceAgent(agent, {\n  sttProvider: new OpenAIWhisperProvider(),\n  ttsProvider: new OpenAITTSProvider({ voice: 'nova' }),\n  autoSpeak: true, // Automatically convert responses to speech\n});\n\n// Voice in ‚Üí Voice out\nconst audioFile = fs.readFileSync('./user-question.mp3');\nconst result = await voiceAgent.processVoice(audioFile, context);\n\n// Save voice response\nfs.writeFileSync('./agent-response.mp3', result.audio);","useCases":["Voice assistants","Accessibility applications","Hands-free interfaces"]}]}],"$L6","$L7"]}]}],null,"$L8"]}],{},null,false]},null,false]},null,false],"$L9",false]],"m":"$undefined","G":["$a",[]],"s":false,"S":true}
b:I[2619,["619","static/chunks/619-ba102abea3e3d0e4.js","92","static/chunks/app/examples/page-339de7a5895506d2.js"],""]
c:I[4431,[],"OutletBoundary"]
e:I[5278,[],"AsyncMetadataOutlet"]
10:I[4431,[],"ViewportBoundary"]
12:I[4431,[],"MetadataBoundary"]
13:"$Sreact.suspense"
6:["$","div",null,{"className":"mt-16 bg-gradient-to-r from-blue-50 to-purple-50 border border-blue-200 rounded-lg p-6 py-32","children":[["$","h2",null,{"className":"text-3xl font-bold mb-4","children":"Ready to Get Started?"}],["$","p",null,{"className":"text-xl mb-6 opacity-90","children":"Check out our comprehensive documentation to learn more about building with AgentSea."}],["$","div",null,{"className":"flex flex-col sm:flex-row gap-4","children":[["$","$Lb",null,{"href":"/docs/quick-start","className":"px-6 py-3 bg-white text-blue-600 rounded-lg font-semibold hover:bg-gray-100 transition-colors text-center","children":"Quick Start Guide"}],["$","$Lb",null,{"href":"/docs","className":"px-6 py-3 bg-blue-700 hover:bg-blue-800 text-white rounded-lg font-semibold transition-colors text-center","children":"Full Documentation"}]]}]]}]
7:["$","div",null,{"className":"mt-16","children":[["$","h2",null,{"className":"text-3xl font-bold mb-8","children":"Additional Resources"}],["$","div",null,{"className":"grid grid-cols-1 md:grid-cols-3 gap-6","children":[["$","a","0",{"href":"/docs","className":"p-6 bg-white rounded-lg border border-gray-200 hover:border-blue-500 transition-colors","children":[["$","div",null,{"className":"text-3xl mb-3","children":"üìñ"}],["$","h3",null,{"className":"text-lg font-semibold mb-2","children":"Documentation"}],["$","p",null,{"className":"text-gray-600 text-sm","children":"Comprehensive guides and API references"}]]}],["$","a","1",{"href":"https://github.com/lov3kaizen/agentsea","className":"p-6 bg-white rounded-lg border border-gray-200 hover:border-blue-500 transition-colors","children":[["$","div",null,{"className":"text-3xl mb-3","children":"üêô"}],["$","h3",null,{"className":"text-lg font-semibold mb-2","children":"GitHub Repository"}],["$","p",null,{"className":"text-gray-600 text-sm","children":"View source code and contribute"}]]}],["$","a","2",{"href":"https://github.com/lov3kaizen/agentsea/discussions","className":"p-6 bg-white rounded-lg border border-gray-200 hover:border-blue-500 transition-colors","children":[["$","div",null,{"className":"text-3xl mb-3","children":"üí¨"}],["$","h3",null,{"className":"text-lg font-semibold mb-2","children":"Community"}],["$","p",null,{"className":"text-gray-600 text-sm","children":"Join discussions and get help"}]]}]]}]]}]
8:["$","$Lc",null,{"children":["$Ld",["$","$Le",null,{"promise":"$@f"}]]}]
9:["$","$1","h",{"children":[null,[["$","$L10",null,{"children":"$L11"}],["$","meta",null,{"name":"next-size-adjust","content":""}]],["$","$L12",null,{"children":["$","div",null,{"hidden":true,"children":["$","$13",null,{"fallback":null,"children":"$L14"}]}]}]]}]
11:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
d:null
f:{"metadata":[["$","title","0",{"children":"AgentSea - Unite and Orchestrate AI Agents"}],["$","meta","1",{"name":"description","content":"Build powerful agentic AI applications with AgentSea. Unite AI agents and services with multi-provider support, workflow orchestration, MCP protocol, and enterprise-grade observability."}]],"error":null,"digest":"$undefined"}
14:"$f:metadata"
